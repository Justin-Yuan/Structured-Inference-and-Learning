{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "batch_size = 200 \n",
    "num_classes = 2\n",
    "state_size = 16\n",
    "learning_rate = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5 \n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5 \n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data \n",
    "    data_length = len(raw_x)\n",
    "    \n",
    "    batch_partition_length = data_length // batch_size \n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i: batch_partition_length * (i+1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i: batch_partition_length * (i+1)]\n",
    "    \n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i+1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i+1) * num_steps]\n",
    "        yield (x, y)\n",
    "        \n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# RNN Inputs \n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "# rnn inputs as a list of a number (batch size) of one hot vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of rnn_cell \n",
    "\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding rnn_cells to graph \n",
    "\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction, loss, training step \n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \n",
    "         logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step =  tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the network \n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses, total_loss, final_state, train_step],\n",
    "                            feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step, \n",
    "                             \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "    return training_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.594163248539\n",
      "Average loss at step 200 for last 250 steps: 0.521576545238\n",
      "Average loss at step 300 for last 250 steps: 0.521482703388\n",
      "Average loss at step 400 for last 250 steps: 0.519919075072\n",
      "\n",
      "EPOCH 1\n",
      "Average loss at step 100 for last 250 steps: 0.525433504879\n",
      "Average loss at step 200 for last 250 steps: 0.519083153903\n",
      "Average loss at step 300 for last 250 steps: 0.520424753129\n",
      "Average loss at step 400 for last 250 steps: 0.520264916122\n",
      "\n",
      "EPOCH 2\n",
      "Average loss at step 100 for last 250 steps: 0.523283072114\n",
      "Average loss at step 200 for last 250 steps: 0.521473294199\n",
      "Average loss at step 300 for last 250 steps: 0.519357462525\n",
      "Average loss at step 400 for last 250 steps: 0.520110862553\n",
      "\n",
      "EPOCH 3\n",
      "Average loss at step 100 for last 250 steps: 0.526946133077\n",
      "Average loss at step 200 for last 250 steps: 0.518657598495\n",
      "Average loss at step 300 for last 250 steps: 0.522180112302\n",
      "Average loss at step 400 for last 250 steps: 0.51898521632\n",
      "\n",
      "EPOCH 4\n",
      "Average loss at step 100 for last 250 steps: 0.524537058771\n",
      "Average loss at step 200 for last 250 steps: 0.518107903898\n",
      "Average loss at step 300 for last 250 steps: 0.518709896505\n",
      "Average loss at step 400 for last 250 steps: 0.519496857226\n",
      "\n",
      "EPOCH 5\n",
      "Average loss at step 100 for last 250 steps: 0.524371391535\n",
      "Average loss at step 200 for last 250 steps: 0.518634754121\n",
      "Average loss at step 300 for last 250 steps: 0.519912477732\n",
      "Average loss at step 400 for last 250 steps: 0.519664742053\n",
      "\n",
      "EPOCH 6\n",
      "Average loss at step 100 for last 250 steps: 0.523702079952\n",
      "Average loss at step 200 for last 250 steps: 0.519766471088\n",
      "Average loss at step 300 for last 250 steps: 0.521216062009\n",
      "Average loss at step 400 for last 250 steps: 0.52006775409\n",
      "\n",
      "EPOCH 7\n",
      "Average loss at step 100 for last 250 steps: 0.525863004327\n",
      "Average loss at step 200 for last 250 steps: 0.521147735715\n",
      "Average loss at step 300 for last 250 steps: 0.521938909292\n",
      "Average loss at step 400 for last 250 steps: 0.519947238564\n",
      "\n",
      "EPOCH 8\n",
      "Average loss at step 100 for last 250 steps: 0.524387287498\n",
      "Average loss at step 200 for last 250 steps: 0.520062941611\n",
      "Average loss at step 300 for last 250 steps: 0.522201566696\n",
      "Average loss at step 400 for last 250 steps: 0.518951943219\n",
      "\n",
      "EPOCH 9\n",
      "Average loss at step 100 for last 250 steps: 0.525541529357\n",
      "Average loss at step 200 for last 250 steps: 0.520083411038\n",
      "Average loss at step 300 for last 250 steps: 0.518922785819\n",
      "Average loss at step 400 for last 250 steps: 0.519448004365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x115b2b7f0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMVHGEr8xxkSZ/w905kBIAAtT2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115b9dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(10, num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is non-sense \n",
    "import basic_rnn \n",
    "def plot_learning_curve(num_steps, state_size=4, epochs=1):\n",
    "    global losses, total_loss, final_state, train_step, x, y, init_state\n",
    "    tf.reset_default_graph()\n",
    "    g = tf.get_default_graph()\n",
    "    losses, total_loss, final_state, train_step, x, y, init_state = \\\n",
    "        basic_rnn.setup_graph(g, \n",
    "            basic_rnn.RNN_config(num_steps=num_steps, state_size=state_size))\n",
    "    res = train_network(epochs, num_steps, state_size=state_size, verbose=False)\n",
    "    plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/zhaocongyuan/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    492\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaocongyuan/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaocongyuan/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         % (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    590\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"one_hot:0\", shape=(200, ?, 2), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0bcfd4db3966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNUM_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-c01a15fa3855>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(num_steps, state_size, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     losses, total_loss, final_state, train_step, x, y, init_state =         basic_rnn.setup_graph(g, \n\u001b[0;32m----> 7\u001b[0;31m             basic_rnn.RNN_config(num_steps=num_steps, state_size=state_size))\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaocongyuan/Desktop/research/Structured-Inference-and-Learning/notes/RNN/intro_to_RNN/tut_series2/basic_rnn.py\u001b[0m in \u001b[0;36msetup_graph\u001b[0;34m(graph, config)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_placeholder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'labels_placeholder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdefault_init_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder_with_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_init_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'state_placeholder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaocongyuan/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1214\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_or_size_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m     return gen_array_ops._split(\n\u001b[0;32m-> 1216\u001b[0;31m         split_dim=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001b[0m\u001b[1;32m   1217\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m     \u001b[0msize_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_or_size_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaocongyuan/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_split\u001b[0;34m(split_dim, value, num_split, name)\u001b[0m\n\u001b[1;32m   3424\u001b[0m   \"\"\"\n\u001b[1;32m   3425\u001b[0m   result = _op_def_lib.apply_op(\"Split\", split_dim=split_dim, value=value,\n\u001b[0;32m-> 3426\u001b[0;31m                                 num_split=num_split, name=name)\n\u001b[0m\u001b[1;32m   3427\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaocongyuan/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[0;32m--> 509\u001b[0;31m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m               \u001b[0;31m# Update the maps with the default, if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32."
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "NUM_STEPS = 1\n",
    "\"\"\"\n",
    "plot_learning_curve(num_steps=1, state_size=4, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The above is deprecated !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow API Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace definition of rnn_cell & adding rnn_cells to graph into the following \n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dynamic RNN construction is preferred over static construction \n",
    "\n",
    "# replace the following \n",
    "# x_one_hot = tf.one_hot(x, num_classes)\n",
    "# rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "# replace definition of rnn_cell & adding rnn_cells to graph into the following \n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_input, initial_state=init_state)\n",
    "\n",
    "# replace the following \n",
    "# logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "# predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "# losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "#           logit, label in zip(logits, y_as_list)]\n",
    "# total_loss = tf.reduce_mean(losses)\n",
    "# train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "logits = tf.reshape(\n",
    "            tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "            [batch_size, num_steps, num_classes])\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
