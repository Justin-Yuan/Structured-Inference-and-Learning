{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset (without word embedding and one-hot labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_num = 8\n",
    "\n",
    "labels = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "label_dict = {labels[i]: i for i in range(8)}\n",
    "index_dict = {i: labels[i] for i in range(8)}\n",
    "print(label_dict)\n",
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.load('corpus_all_9999.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve raw datasets (without embeddings) from the data file\n",
    "\n",
    "train_doc = f['train_doc']\n",
    "valid_doc = f['valid_doc']\n",
    "test_doc = f['test_doc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check classes distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_distribution(data, index_dict):\n",
    "    d = {index_dict[key]:0 for key in index_dict}\n",
    "    for doc in data:\n",
    "        d[index_dict[doc[1]]] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classes_distri = get_class_distribution(train_doc, index_dict)\n",
    "print(train_classes_distri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_classes_distri = get_class_distribution(valid_doc, index_dict)\n",
    "print(validation_classes_distri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_classes_distri = get_class_distribution(test_doc, index_dict)\n",
    "print(test_classes_distri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API for corpuse format conversion \n",
    "\n",
    "def map_word_and_index(input_doc, top=10000):\n",
    "    \"\"\" construct words and indices mapping dictionaries \n",
    "    \"\"\"\n",
    "    counts_new = []            \n",
    "    words_count_list = Counter([word for doc in input_doc for sent in doc[0] for word in sent]).most_common(top)\n",
    "    word2index = {item[0]: index for index, item in enumerate(words_count_list, 1)}\n",
    "    index2word = {index: item[0] for index, item in enumerate(words_count_list, 1)}\n",
    "    return word2index, index2word\n",
    "\n",
    "def convert2words(doc):\n",
    "    \"\"\" flatten a document into a list of words \n",
    "    \"\"\"\n",
    "    return [word for sent in doc for word in sent]\n",
    "\n",
    "def doc2index(doc, word2index):\n",
    "    \"\"\" map words to indices for the flatten document \n",
    "    \"\"\"\n",
    "    return [[word2index[word]] for word in doc]\n",
    "\n",
    "def convert_corpus(corpus, word2index):\n",
    "    corpus_words = [(convert2words(doc[0]), doc[1]) for doc in corpus]\n",
    "    return np.asarray([doc2index(doc[0], word2index) for doc in corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index, index2word = map_word_and_index(train_dc, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datasets with each document as a list of word indices (no sentence structure)\n",
    "\n",
    "train_wordasindex = convert_corpus(train_doc, word2index)\n",
    "valid_wordasindex = convert_corpus(valid_doc, word2index)\n",
    "test_wordasindex = convert_corpus(test_doc, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_from_index(model, idx):\n",
    "    \"\"\" apply embedding to a word represented as an index \n",
    "    \"\"\"\n",
    "    if idx != 0:\n",
    "        return model.wv[index2word[idx]]\n",
    "    else:\n",
    "        return np.zeros(shape=model.wv[index2word[1]].shape)\n",
    "\n",
    "def embed_corpus(corpus, word2index, model):\n",
    "    \"\"\" apply word embedding to the dataset \n",
    "    \"\"\"\n",
    "    temp = list(corpus)\n",
    "    return np.asarray([[embed_from_index(model, idx) for sent in doc for idx in sent] for doc in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embed words with trained Word2Vec model (with sentence structure)\n",
    "# This part is only for hierarchical model which is subject to future exploration\n",
    "\n",
    "# train_wordasindex_embed = embed_corpus(train_doc, word2index, model)\n",
    "\n",
    "# valid_wordasindex_embed = embed_corpus(valid_doc, word2index, model)\n",
    "\n",
    "# test_wordasindex_embed = embed_corpus(test_doc, word2index, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_label(label, size):\n",
    "    \"\"\" one-hot encode the given label \n",
    "    \"\"\"\n",
    "    l = [0]*size\n",
    "    l[label] = 1\n",
    "    return l\n",
    "\n",
    "def encode_class(corpus, size):\n",
    "    \"\"\" apply one-hot encoding to the dataset labels \n",
    "    \"\"\"\n",
    "    return np.asarray([encode_label(doc[1], size) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = encode_class(train_doc, 8)\n",
    "valid_label = encode_class(valid_doc, 8)\n",
    "test_label = encode_class(test_doc, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "6686\n"
     ]
    }
   ],
   "source": [
    "# maximum doc length in terms of number of words\n",
    "\n",
    "min_count = 100\n",
    "max_count = 0\n",
    "\n",
    "for doc in valid_wordasindex:\n",
    "    if len(doc) < min_count:\n",
    "        min_count = len(doc)\n",
    "    if len(doc) > max_count:\n",
    "        max_count = len(doc)\n",
    "\n",
    "print(min_count)\n",
    "print(max_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the flattened dataset and reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('corpus_wordasindex_all_ 9999', train_wordasindex=train_wordasindex,\n",
    "         valid_wordasindex=valid_wordasindex, test_wordasindex=test_wordasindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload the flattened dataset, use it after you save it once \n",
    "\n",
    "d = np.load('corpus_wordasindex_all_ 9999')\n",
    "\n",
    "train_wordasindex = d['train_wordasindex']\n",
    "valid_wordasindex = d['valid_wordasindex']\n",
    "test_wordasindex = d['test_wordasindex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "learning_rate = 0.001\n",
    "maxlen = 6700\n",
    "batch_size = 50\n",
    "total_batch = int(train_wordasindex.shape[0]/batch_size)\n",
    "input_dims = 100\n",
    "num_hidden= 50\n",
    "epochs = 100\n",
    "\n",
    "index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_with_zeros(sequence, maxlen):\n",
    "    \"\"\" pad text body with preceding 0s \n",
    "    \"\"\"\n",
    "    if len(sequence) > maxlen:\n",
    "        raise Exception\n",
    "    else:\n",
    "        return [[0]]*(maxlen-len(sequence))+sequence\n",
    "\n",
    "def corpus_pad_with_zeros(corpus, maxlen):\n",
    "    \"\"\" apply zero padding for the whole dataset \n",
    "    \"\"\"\n",
    "    return np.asarray([pad_with_zeros(sent, maxlen) for sent in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero padding on the texts\n",
    "\n",
    "train_corpus = corpus_pad_with_zeros(train_wordasindex, maxlen)\n",
    "\n",
    "valid_corpus = corpus_pad_with_zeros(valid_wordasindex, maxlen)\n",
    "\n",
    "test_corpus = corpus_pad_with_zeros(test_wordasindex, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embed the dataset (without sentence structure)\n",
    "\n",
    "train_corpus_embed = embed_corpus(train_corpus, word2index, model)\n",
    "\n",
    "valid_corpus_embed = embed_corpus(valid_corpus, word2index, model)\n",
    "\n",
    "test_corpus_embed = embed_corpus(test_corpus, word2index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data, index, size):\n",
    "    \"\"\" return next batch in format: index, x batch, y batch\n",
    "    \"\"\"\n",
    "    if index + size <= data[0].shape[0]:\n",
    "        return index+size, data[0][index:index+size], data[1][index:index+size]\n",
    "    else:\n",
    "        return index+size-data[0].shape[0], np.concatenate((data[0][index:],data[0][:index+size-data[0].shape[0]]), 0), \\\n",
    "    np.concatenate((data[1][index:],data[1][:index+size-data[1].shape[0]]), 0)\n",
    "\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For recurrent neural networks, tensorflow wants a data format of [Batch Size, Sequence Length, Input Dimension]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, maxlen, input_dims])\n",
    "y = tf.placeholder(tf.float32, [None, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a LSTM cell, simply call the \"tf.contrib.rnn.LSTMCell\" function with given arguments for size of the hidden state, \"state_is_tuple=True\" will get both the hidden state and the cell state.\n",
    "\n",
    "You can also easily construct RNN variants, for example, call \"tf.conctrib.rnn.GRUCell\" for a GRU network, all the rest is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cell_def', reuse=True):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    # cell_bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a forward LSTM network, call \"tf.nn.dynamic_rnn\" with the constructed LSTM cell and input tensor, specifying \"sequence_length\" will dynamically unroll the network to a matching length to the current input during computation. \"val\" will be a sequence of outputs and \"state\" will be the last hidden state from the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('lstm_def'):\n",
    "    val, state = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=length(x))    # val are the hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply nonlinear transformation to the LSTM output (we choose mean of the hidden states here, you can try out different variants such as the final hidden state, with or without transformation, etc) and render for classification using softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[50, 32]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[32, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "# dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(tf.reduce_mean(val, axis=1), W) + b)\n",
    "# h = tf.nn.relu(tf.matmul(state[1], W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate model saver to save the model in a session \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, run 0, loss 0.889945\n",
      "epoch 0, run 10, loss 0.940488\n",
      "epoch 0, run 20, loss 0.737024\n",
      "epoch 0, run 30, loss 1.28468\n",
      "epoch 0, Validation acc: 44.4%\n",
      "epoch 1, run 0, loss 0.989343\n",
      "epoch 1, run 10, loss 0.927828\n",
      "epoch 1, run 20, loss 0.820058\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss, acc_train = sess.run([optimizer, loss, accuracy], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        acc_val = sess.run(accuracy, feed_dict={x:valid_corpus_embed, y:valid_label})\n",
    "        print(\"epoch %d, Training acc: %g, Validation acc: %g \" % (i, acc_train, acc_val))\n",
    "        \n",
    "save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"models/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={x:test_corpus_embed, y:test_label})\n",
    "    print(\"Test acc: %g\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the benefits and downsides of the RNN-based representation over the bag of words representation used last week? \n",
    "- How would availability of data affect your answer?\n",
    "- One possible architectural variant is to use only the final hidden state of the RNN as the document representation (i.e., x) rather than the average of the hidden states over time. How does this work? What are the potential benefits and downsides to this representation?\n",
    "- Try different RNN architectures, e.g., simple Elman RNNs or GRUs or LSTMs. Which ones work best?\n",
    "- What happens if you use a bidirectional LSTM (i.e., the dashed arrows in the figure)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, maxlen, input_dims])\n",
    "y = tf.placeholder(tf.float32, [None, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM needs a forward LSTM cell and a backward LSTM cell (they are the same here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cells_def', reuse=True):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a bidirectional LSTM network is as easy as calling the \"tf.bidirectional_dynamic_rnn\" function with the foward and backward cells, the returned outputs and states are both a tuple of two, each contains the sequence of output states and the final hidden state for the corresponding direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('bidirlstm_def'):\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x, dtype=tf.float32, sequence_length=length(x))    # val are the h_ts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose how to use the outputs and hidden states from both directions. Here we are only using the output states and we take the mean between the forward and backward directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = (outputs[0] + outputs[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you should get the new representation of the document, classify it through further nonlinear transformation and softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[50, 32]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[32, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "# dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(tf.reduce_mean(val, axis=1), W) + b)\n",
    "# h = tf.nn.relu(tf.matmul(state[1], W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss, acc_train = sess.run([optimizer, loss, accuracy], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        acc_val = sess.run(accuracy, feed_dict={x:valid_corpus_embed, y:valid_label})\n",
    "        print(\"epoch %d, Training acc: %g, Validation acc: %g \" % (i, acc_train, acc_val))\n",
    "        \n",
    "save_path = saver.save(sess, \"models/bidir_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"models/bidir_model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={x:test_corpus_embed, y:test_label})\n",
    "    print(\"Test acc: %g\" % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
