{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_num = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = np.load('corpus_all_9999.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_doc = f['train_doc']\n",
    "valid_doc = f['valid_doc']\n",
    "test_doc = f['test_doc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API for corpuse format conversion \n",
    "\n",
    "def map_word_and_index(input_doc, top=10000):\n",
    "    counts_new = []            \n",
    "    words_count_list = Counter([word for doc in input_doc for sent in doc[0] for word in sent]).most_common(top)\n",
    "    word2index = {item[0]: index for index, item in enumerate(words_count_list, 1)}\n",
    "    index2word = {index: item[0] for index, item in enumerate(words_count_list, 1)}\n",
    "    return word2index, index2word\n",
    "\n",
    "def convert2words(doc):\n",
    "    return [word for sent in doc for word in sent]\n",
    "\n",
    "def doc2index(doc, word2index):\n",
    "    return [[word2index[word]] for word in doc]\n",
    "\n",
    "def convert_corpus(corpus, word2index):\n",
    "    corpus_words = [(convert2words(doc[0]), doc[1]) for doc in corpus]\n",
    "    return np.asarray([doc2index(doc[0], word2index) for doc in corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index, index2word = map_word_and_index(train_dc, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wordasindex = convert_corpus(train_doc, word2index)\n",
    "valid_wordasindex = convert_corpus(valid_doc, word2index)\n",
    "test_wordasindex = convert_corpus(test_doc, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_from_index(model, idx):\n",
    "    if idx != 0:\n",
    "        return model.wv[index2word[idx]]\n",
    "    else:\n",
    "        return np.zeros(shape=model.wv[index2word[1]].shape)\n",
    "\n",
    "def embed_corpus(corpus, word2index, model):\n",
    "    temp = list(corpus)\n",
    "    return np.asarray([[embed_from_index(model, idx) for sent in doc for idx in sent] for doc in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wordasindex_embed = embed_corpus(train_doc, word2index, model)\n",
    "valid_wordasindex_embed = embed_corpus(valid_doc, word2index, model)\n",
    "test_wordasindex_embed = embed_corpus(test_doc, word2index, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_label(label, size):\n",
    "    l = [0]*size\n",
    "    l[label] = 1\n",
    "    return l\n",
    "\n",
    "def encode_class(corpus, size):\n",
    "    return np.asarray([encode_label(doc[1], size) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = encode_class(train_doc, 8)\n",
    "valid_label = encode_class(valid_doc, 8)\n",
    "test_label = encode_class(test_doc, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "6686\n"
     ]
    }
   ],
   "source": [
    "# maximum doc length in terms of number of words\n",
    "\n",
    "min_count = 100\n",
    "max_count = 0\n",
    "\n",
    "for doc in valid_wordasindex:\n",
    "    if len(doc) < min_count:\n",
    "        min_count = len(doc)\n",
    "    if len(doc) > max_count:\n",
    "        max_count = len(doc)\n",
    "\n",
    "print(min_count)\n",
    "print(max_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('corpus_wordasindex_all_ 9999', train_wordasindex=train_wordasindex,\n",
    "         valid_wordasindex_embed=v, test_wordasindex=test_wordasindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "learning_rate = 0.001\n",
    "maxlen = 6700\n",
    "batch_size = 50\n",
    "total_batch = int(train_wordasindex.shape[0]/batch_size)\n",
    "input_dims = 100\n",
    "num_hidden= 50\n",
    "epochs = 100\n",
    "\n",
    "index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_with_zeros(sequence, maxlen):\n",
    "    if len(sequence) > maxlen:\n",
    "        raise Exception\n",
    "    else:\n",
    "        return sequence+[[0]]*(maxlen-len(sequence))\n",
    "\n",
    "def corpus_pad_with_zeros(corpus, maxlen):\n",
    "    return np.asarray([pad_with_zeros(sent, maxlen) for sent in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = corpus_pad_with_zeros(train_wordasindex, maxlen)\n",
    "valid_corpus = corpus_pad_with_zeros(valid_wordasindex, maxlen)\n",
    "test_corpus = corpus_pad_with_zeros(test_wordasindex, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_corpus_embed = embed_corpus(train_corpus, word2index, model)\n",
    "valid_corpus_embed = embed_corpus(valid_corpus, word2index, model)\n",
    "test_corpus_embed = embed_corpus(test_corpus, word2index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data, index, size):\n",
    "    \"\"\" return next batch in format: index, x batch, y batch\n",
    "    \"\"\"\n",
    "    if index + size <= data[0].shape[0]:\n",
    "        return index+size, data[0][index:index+size], data[1][index:index+size]\n",
    "    else:\n",
    "        return index+size-data[0].shape[0], np.concatenate((data[0][index:],data[0][:index+size-data[0].shape[0]]), 0), \\\n",
    "    np.concatenate((data[1][index:],data[1][:index+size-data[1].shape[0]]), 0)\n",
    "\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow wants a data format of [Batch Size, Sequence Length, Input Dimension].\n",
    "x = tf.placeholder(tf.float32, [None, maxlen, input_dims])\n",
    "y = tf.placeholder(tf.float32, [None, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cellsdef', reuse=True):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('lstmrnn'):\n",
    "    val, state = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=length(x))    # val are the h_ts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[50, 32]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[32, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "# dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(tf.reduce_mean(val, axis=1), W) + b)\n",
    "# h = tf.nn.relu(tf.matmul(state[1], W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, run 0, loss 0.889945\n",
      "epoch 0, run 10, loss 0.940488\n",
      "epoch 0, run 20, loss 0.737024\n",
      "epoch 0, run 30, loss 1.28468\n",
      "epoch 0, Validation acc: 44.4%\n",
      "epoch 1, run 0, loss 0.989343\n",
      "epoch 1, run 10, loss 0.927828\n",
      "epoch 1, run 20, loss 0.820058\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    acc = 0.0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss = sess.run([optimizer, loss], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict={x:valid_corpus_embed, y:valid_label})\n",
    "        print(\"epoch %d, Validation acc: %g\" % (i, acc * 100), end=\"\")\n",
    "        print(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a1209de60ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5ceac6cf84dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    acc = 0.0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss = sess.run([optimizer, loss], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict={x:test_corpus_embed, y:test_label})\n",
    "        print(\"epoch %d, Test acc: %g\" % (i, acc * 100), end=\"\")\n",
    "        print(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the benefits and downsides of the RNN-based representation over the bag of words representation used last week? \n",
    "- How would availability of data affect your answer?\n",
    "- One possible architectural variant is to use only the final hidden state of the RNN as the document representation (i.e., x) rather than the average of the hidden states over time. How does this work? What are the potential benefits and downsides to this representation?\n",
    "- Try different RNN architectures, e.g., simple Elman RNNs or GRUs or LSTMs. Which ones work best?\n",
    "- What happens if you use a bidirectional LSTM (i.e., the dashed arrows in the figure)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
