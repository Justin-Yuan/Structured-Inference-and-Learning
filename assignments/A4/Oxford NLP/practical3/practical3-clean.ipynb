{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset (without word embedding and one-hot labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ooo': 0, 'Too': 1, 'oEo': 2, 'ooD': 3, 'TEo': 4, 'ToD': 5, 'oED': 6, 'TED': 7}\n",
      "{0: 'ooo', 1: 'Too', 2: 'oEo', 3: 'ooD', 4: 'TEo', 5: 'ToD', 6: 'oED', 7: 'TED'}\n"
     ]
    }
   ],
   "source": [
    "label_num = 8\n",
    "\n",
    "labels = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "label_dict = {labels[i]: i for i in range(8)}\n",
    "index_dict = {i: labels[i] for i in range(8)}\n",
    "print(label_dict)\n",
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.load('corpus_all_9999.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve raw datasets (without embeddings) from the data file\n",
    "\n",
    "train_doc = f['train_doc']\n",
    "valid_doc = f['valid_doc']\n",
    "test_doc = f['test_doc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check classes distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_distribution(data, index_dict):\n",
    "    d = {index_dict[key]:0 for key in index_dict}\n",
    "    for doc in data:\n",
    "        d[index_dict[doc[1]]] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ooo': 847, 'Too': 295, 'oEo': 132, 'ooD': 116, 'TEo': 24, 'ToD': 119, 'oED': 20, 'TED': 27}\n"
     ]
    }
   ],
   "source": [
    "train_classes_distri = get_class_distribution(train_doc, index_dict)\n",
    "print(train_classes_distri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ooo': 139, 'Too': 46, 'oEo': 17, 'ooD': 24, 'TEo': 4, 'ToD': 12, 'oED': 2, 'TED': 4}\n"
     ]
    }
   ],
   "source": [
    "validation_classes_distri = get_class_distribution(valid_doc, index_dict)\n",
    "print(validation_classes_distri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ooo': 139, 'Too': 40, 'oEo': 21, 'ooD': 24, 'TEo': 7, 'ToD': 14, 'oED': 2, 'TED': 3}\n"
     ]
    }
   ],
   "source": [
    "test_classes_distri = get_class_distribution(test_doc, index_dict)\n",
    "print(test_classes_distri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5360759493670886\n",
      "0.18670886075949367\n",
      "0.08354430379746836\n",
      "0.07341772151898734\n",
      "0.015189873417721518\n",
      "0.07531645569620253\n",
      "0.012658227848101266\n",
      "0.01708860759493671\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key in train_classes_distri:\n",
    "    count += train_classes_distri[key]\n",
    "    \n",
    "for key in train_classes_distri:\n",
    "    print(train_classes_distri[key]/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API for corpuse format conversion \n",
    "\n",
    "def map_word_and_index(input_doc, top=10000):\n",
    "    \"\"\" construct words and indices mapping dictionaries \n",
    "    \"\"\"\n",
    "    counts_new = []            \n",
    "    words_count_list = Counter([word for doc in input_doc for sent in doc[0] for word in sent]).most_common(top)\n",
    "    word2index = {item[0]: index for index, item in enumerate(words_count_list, 1)}\n",
    "    index2word = {index: item[0] for index, item in enumerate(words_count_list, 1)}\n",
    "    return word2index, index2word\n",
    "\n",
    "def convert2words(doc):\n",
    "    \"\"\" flatten a document into a list of words \n",
    "    \"\"\"\n",
    "    return [word for sent in doc for word in sent]\n",
    "\n",
    "def doc2index(doc, word2index):\n",
    "    \"\"\" map words to indices for the flatten document \n",
    "    \"\"\"\n",
    "    return [[word2index[word]] for word in doc]\n",
    "\n",
    "def convert_corpus(corpus, word2index):\n",
    "    corpus_words = [(convert2words(doc[0]), doc[1]) for doc in corpus]\n",
    "    return np.asarray([doc2index(doc[0], word2index) for doc in corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2index, index2word = map_word_and_index(train_doc, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datasets with each document as a list of word indices (no sentence structure)\n",
    "\n",
    "train_wordasindex = convert_corpus(train_doc, word2index)\n",
    "valid_wordasindex = convert_corpus(valid_doc, word2index)\n",
    "test_wordasindex = convert_corpus(test_doc, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_from_index(model, idx):\n",
    "    \"\"\" apply embedding to a word represented as an index \n",
    "    \"\"\"\n",
    "    if idx != 0:\n",
    "        return model.wv[index2word[idx]]\n",
    "    else:\n",
    "        return np.zeros(shape=model.wv[index2word[1]].shape)\n",
    "\n",
    "def embed_corpus(corpus, word2index, model):\n",
    "    \"\"\" apply word embedding to the dataset \n",
    "    \"\"\"\n",
    "    temp = list(corpus)\n",
    "    return np.asarray([[embed_from_index(model, idx) for sent in doc for idx in sent] for doc in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embed words with trained Word2Vec model (with sentence structure)\n",
    "# This part is only for hierarchical model which is subject to future exploration\n",
    "\n",
    "# train_wordasindex_embed = embed_corpus(train_doc, word2index, model)\n",
    "\n",
    "# valid_wordasindex_embed = embed_corpus(valid_doc, word2index, model)\n",
    "\n",
    "# test_wordasindex_embed = embed_corpus(test_doc, word2index, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_label(label, size):\n",
    "    \"\"\" one-hot encode the given label \n",
    "    \"\"\"\n",
    "    l = [0]*size\n",
    "    l[label] = 1\n",
    "    return l\n",
    "\n",
    "def encode_class(corpus, size):\n",
    "    \"\"\" apply one-hot encoding to the dataset labels \n",
    "    \"\"\"\n",
    "    return np.asarray([encode_label(doc[1], size) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = encode_class(train_doc, 8)\n",
    "valid_label = encode_class(valid_doc, 8)\n",
    "test_label = encode_class(test_doc, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "6017\n"
     ]
    }
   ],
   "source": [
    "# maximum doc length in terms of number of words\n",
    "\n",
    "min_count = 100\n",
    "max_count = 0\n",
    "\n",
    "for doc in valid_wordasindex:\n",
    "    if len(doc) < min_count:\n",
    "        min_count = len(doc)\n",
    "    if len(doc) > max_count:\n",
    "        max_count = len(doc)\n",
    "\n",
    "print(min_count)\n",
    "print(max_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the flattened dataset and reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('corpus_wordasindex_all_9999', train_wordasindex=train_wordasindex,\n",
    "         valid_wordasindex=valid_wordasindex, test_wordasindex=test_wordasindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload the flattened dataset, use it after you save it once \n",
    "\n",
    "d = np.load('corpus_wordasindex_all_9999.npz')\n",
    "\n",
    "train_wordasindex = d['train_wordasindex']\n",
    "valid_wordasindex = d['valid_wordasindex']\n",
    "test_wordasindex = d['test_wordasindex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "learning_rate = 0.001\n",
    "maxlen = 6700\n",
    "batch_size = 50\n",
    "total_batch = int(train_wordasindex.shape[0]/batch_size)\n",
    "input_dims = 100\n",
    "num_hidden= 50\n",
    "epochs = 100\n",
    "\n",
    "index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_with_zeros(sequence, maxlen):\n",
    "    \"\"\" pad text body with preceding 0s \n",
    "    \"\"\"\n",
    "    if len(sequence) > maxlen:\n",
    "        raise Exception\n",
    "    else:\n",
    "        return [[0]]*(maxlen-len(sequence))+sequence\n",
    "\n",
    "def corpus_pad_with_zeros(corpus, maxlen):\n",
    "    \"\"\" apply zero padding for the whole dataset \n",
    "    \"\"\"\n",
    "    return np.asarray([pad_with_zeros(sent, maxlen) for sent in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero padding on the texts\n",
    "\n",
    "train_corpus = corpus_pad_with_zeros(train_wordasindex, maxlen)\n",
    "\n",
    "valid_corpus = corpus_pad_with_zeros(valid_wordasindex, maxlen)\n",
    "\n",
    "test_corpus = corpus_pad_with_zeros(test_wordasindex, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embed the dataset (without sentence structure)\n",
    "\n",
    "train_corpus_embed = embed_corpus(train_corpus, word2index, model)\n",
    "\n",
    "valid_corpus_embed = embed_corpus(valid_corpus, word2index, model)\n",
    "\n",
    "test_corpus_embed = embed_corpus(test_corpus, word2index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data, index, size):\n",
    "    \"\"\" return next batch in format: index, x batch, y batch\n",
    "    \"\"\"\n",
    "    if index + size <= data[0].shape[0]:\n",
    "        return index+size, data[0][index:index+size], data[1][index:index+size]\n",
    "    else:\n",
    "        return index+size-data[0].shape[0], np.concatenate((data[0][index:],data[0][:index+size-data[0].shape[0]]), 0), \\\n",
    "    np.concatenate((data[1][index:],data[1][:index+size-data[1].shape[0]]), 0)\n",
    "\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For recurrent neural networks, tensorflow wants a data format of [Batch Size, Sequence Length, Input Dimension]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, maxlen, input_dims])\n",
    "y = tf.placeholder(tf.float32, [None, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a LSTM cell, simply call the \"tf.contrib.rnn.LSTMCell\" function with given arguments for size of the hidden state, \"state_is_tuple=True\" will get both the hidden state and the cell state.\n",
    "\n",
    "You can also easily construct RNN variants, for example, call \"tf.conctrib.rnn.GRUCell\" for a GRU network, all the rest is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cell_def', reuse=True):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    # cell_bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a forward LSTM network, call \"tf.nn.dynamic_rnn\" with the constructed LSTM cell and input tensor, specifying \"sequence_length\" will dynamically unroll the network to a matching length to the current input during computation. \"val\" will be a sequence of outputs and \"state\" will be the last hidden state from the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('lstm_def'):\n",
    "    val, state = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=length(x))    # val are the hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply nonlinear transformation to the LSTM output (we choose mean of the hidden states here, you can try out different variants such as the final hidden state, with or without transformation, etc) and render for classification using softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[50, 32]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[32, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "# dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(tf.reduce_mean(val, axis=1), W) + b)\n",
    "# h = tf.nn.relu(tf.matmul(state[1], W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate model saver to save the model in a session \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, run 0, loss 2.08369\n",
      "epoch 0, run 10, loss 1.92603\n",
      "epoch 0, run 20, loss 1.80858\n",
      "epoch 0, run 30, loss 1.62792\n",
      "epoch 0, Training acc: 0.58, Validation acc: 0.556452 \n",
      "epoch 1, run 0, loss 1.78788\n",
      "epoch 1, run 10, loss 1.84149\n",
      "epoch 1, run 20, loss 1.70693\n",
      "epoch 1, run 30, loss 1.44119\n",
      "epoch 2, run 0, loss 1.33701\n",
      "epoch 2, run 10, loss 1.58\n",
      "epoch 2, run 20, loss 1.37917\n",
      "epoch 2, run 30, loss 1.14554\n",
      "epoch 2, Training acc: 0.7, Validation acc: 0.556452 \n",
      "epoch 3, run 0, loss 1.51547\n",
      "epoch 3, run 10, loss 1.52064\n",
      "epoch 3, run 20, loss 1.43776\n",
      "epoch 3, run 30, loss 1.27195\n",
      "epoch 4, run 0, loss 1.32246\n",
      "epoch 4, run 10, loss 1.37758\n",
      "epoch 4, run 20, loss 1.37754\n",
      "epoch 4, run 30, loss 1.36137\n",
      "epoch 4, Training acc: 0.54, Validation acc: 0.564516 \n",
      "epoch 5, run 0, loss 1.14413\n",
      "epoch 5, run 10, loss 1.25797\n",
      "epoch 5, run 20, loss 1.34334\n",
      "epoch 5, run 30, loss 1.5049\n",
      "epoch 6, run 0, loss 1.25913\n",
      "epoch 6, run 10, loss 1.45705\n",
      "epoch 6, run 20, loss 1.30053\n",
      "epoch 6, run 30, loss 1.55018\n",
      "epoch 6, Training acc: 0.54, Validation acc: 0.556452 \n",
      "epoch 7, run 0, loss 1.30926\n",
      "epoch 7, run 10, loss 1.53574\n",
      "epoch 7, run 20, loss 1.30754\n",
      "epoch 7, run 30, loss 1.49157\n",
      "epoch 8, run 0, loss 1.53774\n",
      "epoch 8, run 10, loss 1.33088\n",
      "epoch 8, run 20, loss 1.54205\n",
      "epoch 8, run 30, loss 1.48645\n",
      "epoch 8, Training acc: 0.52, Validation acc: 0.560484 \n",
      "epoch 9, run 0, loss 1.46173\n",
      "epoch 9, run 10, loss 1.46542\n",
      "epoch 9, run 20, loss 1.30346\n",
      "epoch 9, run 30, loss 1.48741\n",
      "epoch 10, run 0, loss 1.42317\n",
      "epoch 10, run 10, loss 1.38366\n",
      "epoch 10, run 20, loss 1.36703\n",
      "epoch 10, run 30, loss 1.43976\n",
      "epoch 10, Training acc: 0.44, Validation acc: 0.564516 \n",
      "epoch 11, run 0, loss 1.46845\n",
      "epoch 11, run 10, loss 1.35439\n",
      "epoch 11, run 20, loss 1.45153\n",
      "epoch 11, run 30, loss 1.25058\n",
      "epoch 12, run 0, loss 1.522\n",
      "epoch 12, run 10, loss 1.67391\n",
      "epoch 12, run 20, loss 1.57871\n",
      "epoch 12, run 30, loss 1.41825\n",
      "epoch 12, Training acc: 0.52, Validation acc: 0.564516 \n",
      "epoch 13, run 0, loss 1.32391\n",
      "epoch 13, run 10, loss 1.43889\n",
      "epoch 13, run 20, loss 1.44729\n",
      "epoch 13, run 30, loss 1.55336\n",
      "epoch 14, run 0, loss 1.3106\n",
      "epoch 14, run 10, loss 1.38402\n",
      "epoch 14, run 20, loss 1.46413\n",
      "epoch 14, run 30, loss 1.39307\n",
      "epoch 14, Training acc: 0.48, Validation acc: 0.560484 \n",
      "epoch 15, run 0, loss 1.50286\n",
      "epoch 15, run 10, loss 1.24956\n",
      "epoch 15, run 20, loss 1.36045\n",
      "epoch 15, run 30, loss 1.52311\n",
      "epoch 16, run 0, loss 1.44311\n",
      "epoch 16, run 10, loss 1.19348\n",
      "epoch 16, run 20, loss 1.51076\n",
      "epoch 16, run 30, loss 1.64328\n",
      "epoch 16, Training acc: 0.46, Validation acc: 0.556452 \n",
      "epoch 17, run 0, loss 1.35822\n",
      "epoch 17, run 10, loss 1.52606\n",
      "epoch 17, run 20, loss 1.60389\n",
      "epoch 17, run 30, loss 1.55962\n",
      "epoch 18, run 0, loss 1.50779\n",
      "epoch 18, run 10, loss 1.29065\n",
      "epoch 18, run 20, loss 1.47804\n",
      "epoch 18, run 30, loss 1.41355\n",
      "epoch 18, Training acc: 0.6, Validation acc: 0.556452 \n",
      "epoch 19, run 0, loss 1.50981\n",
      "epoch 19, run 10, loss 1.13931\n",
      "epoch 19, run 20, loss 1.3093\n",
      "epoch 19, run 30, loss 1.28751\n",
      "epoch 20, run 0, loss 1.40579\n",
      "epoch 20, run 10, loss 1.23104\n",
      "epoch 20, run 20, loss 1.3546\n",
      "epoch 20, run 30, loss 1.17543\n",
      "epoch 20, Training acc: 0.64, Validation acc: 0.556452 \n",
      "epoch 21, run 0, loss 1.47631\n",
      "epoch 21, run 10, loss 1.10527\n",
      "epoch 21, run 20, loss 1.25046\n",
      "epoch 21, run 30, loss 1.28442\n",
      "epoch 22, run 0, loss 1.1988\n",
      "epoch 22, run 10, loss 1.13411\n",
      "epoch 22, run 20, loss 1.26531\n",
      "epoch 22, run 30, loss 1.19901\n",
      "epoch 22, Training acc: 0.64, Validation acc: 0.560484 \n",
      "epoch 23, run 0, loss 1.25593\n",
      "epoch 23, run 10, loss 1.28257\n",
      "epoch 23, run 20, loss 1.48803\n",
      "epoch 23, run 30, loss 1.15525\n",
      "epoch 24, run 0, loss 1.23069\n",
      "epoch 24, run 10, loss 1.40489\n",
      "epoch 24, run 20, loss 1.27218\n",
      "epoch 24, run 30, loss 1.37035\n",
      "epoch 24, Training acc: 0.58, Validation acc: 0.556452 \n",
      "epoch 25, run 0, loss 1.2812\n",
      "epoch 25, run 10, loss 1.50138\n",
      "epoch 25, run 20, loss 1.16098\n",
      "epoch 25, run 30, loss 1.41018\n",
      "epoch 26, run 0, loss 1.24473\n",
      "epoch 26, run 10, loss 1.44823\n",
      "epoch 26, run 20, loss 1.53214\n",
      "epoch 26, run 30, loss 1.28346\n",
      "epoch 26, Training acc: 0.58, Validation acc: 0.564516 \n",
      "epoch 27, run 0, loss 1.56805\n",
      "epoch 27, run 10, loss 1.51891\n",
      "epoch 27, run 20, loss 1.31246\n",
      "epoch 27, run 30, loss 1.28502\n",
      "epoch 28, run 0, loss 1.38772\n",
      "epoch 28, run 10, loss 1.39793\n",
      "epoch 28, run 20, loss 1.47041\n",
      "epoch 28, run 30, loss 1.35461\n",
      "epoch 28, Training acc: 0.6, Validation acc: 0.560484 \n",
      "epoch 29, run 0, loss 1.2746\n",
      "epoch 29, run 10, loss 1.39894\n",
      "epoch 29, run 20, loss 1.49264\n",
      "epoch 29, run 30, loss 1.3554\n",
      "epoch 30, run 0, loss 1.43435\n",
      "epoch 30, run 10, loss 1.21754\n",
      "epoch 30, run 20, loss 1.32512\n",
      "epoch 30, run 30, loss 1.38969\n",
      "epoch 30, Training acc: 0.54, Validation acc: 0.544355 \n",
      "epoch 31, run 0, loss 1.50947\n",
      "epoch 31, run 10, loss 1.3415\n",
      "epoch 31, run 20, loss 1.29749\n",
      "epoch 31, run 30, loss 1.31005\n",
      "epoch 32, run 0, loss 1.31576\n",
      "epoch 32, run 10, loss 1.45307\n",
      "epoch 32, run 20, loss 1.13751\n",
      "epoch 32, run 30, loss 1.32706\n",
      "epoch 32, Training acc: 0.58, Validation acc: 0.556452 \n",
      "epoch 33, run 0, loss 1.38241\n",
      "epoch 33, run 10, loss 1.41395\n",
      "epoch 33, run 20, loss 1.29472\n",
      "epoch 33, run 30, loss 1.36652\n",
      "epoch 34, run 0, loss 1.41782\n",
      "epoch 34, run 10, loss 1.3601\n",
      "epoch 34, run 20, loss 1.43778\n",
      "epoch 34, run 30, loss 1.59149\n",
      "epoch 34, Training acc: 0.42, Validation acc: 0.564516 \n",
      "epoch 35, run 0, loss 1.38478\n",
      "epoch 35, run 10, loss 1.59411\n",
      "epoch 35, run 20, loss 1.15551\n",
      "epoch 35, run 30, loss 1.53094\n",
      "epoch 36, run 0, loss 1.51401\n",
      "epoch 36, run 10, loss 1.37031\n",
      "epoch 36, run 20, loss 1.17535\n",
      "epoch 36, run 30, loss 1.22103\n",
      "epoch 36, Training acc: 0.56, Validation acc: 0.568548 \n",
      "epoch 37, run 0, loss 1.58362\n",
      "epoch 37, run 10, loss 1.34966\n",
      "epoch 37, run 20, loss 1.21855\n",
      "epoch 37, run 30, loss 1.14565\n",
      "epoch 38, run 0, loss 1.44056\n",
      "epoch 38, run 10, loss 1.29691\n",
      "epoch 38, run 20, loss 1.17707\n",
      "epoch 38, run 30, loss 1.1117\n",
      "epoch 38, Training acc: 0.68, Validation acc: 0.564516 \n",
      "epoch 39, run 0, loss 1.29476\n",
      "epoch 39, run 10, loss 1.29062\n",
      "epoch 39, run 20, loss 1.17756\n",
      "epoch 39, run 30, loss 1.23913\n",
      "epoch 40, run 0, loss 1.19591\n",
      "epoch 40, run 10, loss 1.12817\n",
      "epoch 40, run 20, loss 1.34769\n",
      "epoch 40, run 30, loss 1.42058\n",
      "epoch 40, Training acc: 0.54, Validation acc: 0.560484 \n",
      "epoch 41, run 0, loss 1.09063\n",
      "epoch 41, run 10, loss 1.14074\n",
      "epoch 41, run 20, loss 1.46516\n",
      "epoch 41, run 30, loss 1.16652\n",
      "epoch 42, run 0, loss 1.35091\n",
      "epoch 42, run 10, loss 1.20652\n",
      "epoch 42, run 20, loss 1.45795\n",
      "epoch 42, run 30, loss 1.17886\n",
      "epoch 42, Training acc: 0.66, Validation acc: 0.568548 \n",
      "epoch 43, run 0, loss 1.3944\n",
      "epoch 43, run 10, loss 1.17779\n",
      "epoch 43, run 20, loss 1.39517\n",
      "epoch 43, run 30, loss 1.29199\n",
      "epoch 44, run 0, loss 1.20345\n",
      "epoch 44, run 10, loss 1.49609\n",
      "epoch 44, run 20, loss 1.40074\n",
      "epoch 44, run 30, loss 1.23753\n",
      "epoch 44, Training acc: 0.6, Validation acc: 0.564516 \n",
      "epoch 45, run 0, loss 1.34159\n",
      "epoch 45, run 10, loss 1.2085\n",
      "epoch 45, run 20, loss 1.38059\n",
      "epoch 45, run 30, loss 1.52702\n",
      "epoch 46, run 0, loss 1.17319\n",
      "epoch 46, run 10, loss 1.20489\n",
      "epoch 46, run 20, loss 1.298\n",
      "epoch 46, run 30, loss 1.44736\n",
      "epoch 46, Training acc: 0.48, Validation acc: 0.560484 \n",
      "epoch 47, run 0, loss 1.19031\n",
      "epoch 47, run 10, loss 1.34637\n",
      "epoch 47, run 20, loss 1.17643\n",
      "epoch 47, run 30, loss 1.23735\n",
      "epoch 48, run 0, loss 1.61357\n",
      "epoch 48, run 10, loss 1.46167\n",
      "epoch 48, run 20, loss 1.37301\n",
      "epoch 48, run 30, loss 1.21776\n",
      "epoch 48, Training acc: 0.62, Validation acc: 0.564516 \n",
      "epoch 49, run 0, loss 1.25414\n",
      "epoch 49, run 10, loss 1.24433\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss, acc_train = sess.run([optimizer, loss, accuracy], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        acc_val = sess.run(accuracy, feed_dict={x:valid_corpus_embed, y:valid_label})\n",
    "        print(\"epoch %d, Training acc: %g, Validation acc: %g \" % (i, acc_train, acc_val))\n",
    "        \n",
    "save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"models/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={x:test_corpus_embed, y:test_label})\n",
    "    print(\"Test acc: %g\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the benefits and downsides of the RNN-based representation over the bag of words representation used last week? \n",
    "- How would availability of data affect your answer?\n",
    "- One possible architectural variant is to use only the final hidden state of the RNN as the document representation (i.e., x) rather than the average of the hidden states over time. How does this work? What are the potential benefits and downsides to this representation?\n",
    "- Try different RNN architectures, e.g., simple Elman RNNs or GRUs or LSTMs. Which ones work best?\n",
    "- What happens if you use a bidirectional LSTM (i.e., the dashed arrows in the figure)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, maxlen, input_dims])\n",
    "y = tf.placeholder(tf.float32, [None, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM needs a forward LSTM cell and a backward LSTM cell (they are the same here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cells_def', reuse=True):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a bidirectional LSTM network is as easy as calling the \"tf.bidirectional_dynamic_rnn\" function with the foward and backward cells, the returned outputs and states are both a tuple of two, each contains the sequence of output states and the final hidden state for the corresponding direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('bidirlstm_def'):\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x, dtype=tf.float32, sequence_length=length(x))    # val are the h_ts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose how to use the outputs and hidden states from both directions. Here we are only using the output states and we take the mean between the forward and backward directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = (outputs[0] + outputs[1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you should get the new representation of the document, classify it through further nonlinear transformation and softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[50, 32]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[32, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "# dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(tf.reduce_mean(val, axis=1), W) + b)\n",
    "# h = tf.nn.relu(tf.matmul(state[1], W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss, acc_train = sess.run([optimizer, loss, accuracy], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 2 == 0:\n",
    "        acc_val = sess.run(accuracy, feed_dict={x:valid_corpus_embed, y:valid_label})\n",
    "        print(\"epoch %d, Training acc: %g, Validation acc: %g \" % (i, acc_train, acc_val))\n",
    "        \n",
    "save_path = saver.save(sess, \"models/bidir_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"models/bidir_model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={x:test_corpus_embed, y:test_label})\n",
    "    print(\"Test acc: %g\" % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
