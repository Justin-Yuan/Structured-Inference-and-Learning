{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 2: Text Classification with Word Embedding\n",
    "<p>Oxford CS - Deep NLP 2017<br>\n",
    "https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/</p>\n",
    "<p>[Yannis Assael, Brendan Shillingford, Chris Dyer]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from random import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"48827e0c-5110-471a-844b-7e270aaf57f9\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"48827e0c-5110-471a-844b-7e270aaf57f9\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"48827e0c-5110-471a-844b-7e270aaf57f9\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '48827e0c-5110-471a-844b-7e270aaf57f9' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"48827e0c-5110-471a-844b-7e270aaf57f9\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"48827e0c-5110-471a-844b-7e270aaf57f9\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TED dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset if it's not already there: this may take a minute as it is 75MB\n",
    "if not os.path.isfile('ted_en-20160408.zip'):\n",
    "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "\n",
    "# get all text body \n",
    "doc_list = doc.xpath('//content/text()')\n",
    "\n",
    "# get all keywords / labels \n",
    "label_list = doc.xpath('//keywords/text()')\n",
    "\n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(keywords, label_dict):\n",
    "    \"\"\" map each document into a proper label from its keywords \n",
    "    \"\"\"\n",
    "    label_string = keywords.lower()\n",
    "    if (\"technology\" in label_string) and (\"entertainment\" in label_string) and (\"design\" in label_string):\n",
    "        return label_dict['TED']\n",
    "    elif (\"entertainment\" in label_string) and (\"design\" in label_string):\n",
    "        return label_dict['oED']\n",
    "    elif (\"technology\" in label_string) and (\"design\" in label_string):\n",
    "        return label_dict['ToD']\n",
    "    elif (\"technology\" in label_string) and (\"entertainment\" in label_string):\n",
    "        return label_dict['TEo']\n",
    "    elif (\"design\" in label_string):\n",
    "        return label_dict['ooD']\n",
    "    elif (\"entertainment\" in label_string):\n",
    "        return label_dict['oEo']\n",
    "    elif (\"technology\" in label_string):\n",
    "        return label_dict['Too']\n",
    "    else:\n",
    "        return label_dict['ooo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proper labels as specified \n",
    "labels = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "label_dict = {labels[i]: i for i in range(8)}\n",
    "\n",
    "# get all proper labels for the documents \n",
    "label_list_temp = [get_label(keywords, label_dict) for keywords in label_list]   \n",
    "\n",
    "# a list of (text body, label) document samples \n",
    "labelled_doc = list(zip(doc_list, label_list_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def divide_dataset(labelled_doc, num_train, num_valid, num_test=None, shuffle=False):\n",
    "    \"\"\" construct training, validation and test set given number of samples \n",
    "    \"\"\"\n",
    "    if num_test == None:\n",
    "        num_test = len(labelled_doc) - num_train - num_valid\n",
    "        \n",
    "    if shuffle:\n",
    "        temp = sample(labelled_doc, len(labelled_doc))\n",
    "    else:\n",
    "        temp = labelled_doc\n",
    "    return temp[:num_train], temp[num_train:-num_test], temp[-num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 1585\n",
      "validation set size: 250\n",
      "test set size 250\n"
     ]
    }
   ],
   "source": [
    "train_doc_temp, valid_doc_temp, test_doc_temp = divide_dataset(labelled_doc, 1585, 250, 250, shuffle=True)\n",
    "\n",
    "print(\"training set size:\",len(train_doc_temp))\n",
    "print(\"validation set size:\", len(valid_doc_temp))\n",
    "print(\"test set size\", len(test_doc_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary using training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def tokenize_and_lowercase(text):\n",
    "    \"\"\" taken from assignment 1,\n",
    "        return a list of sentences, each sentence is a list of words \n",
    "    \"\"\"\n",
    "    text_noparens = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    sentences_strings = []\n",
    "    for line in text_noparens.split('\\n'):\n",
    "        m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "        sentences_strings.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "        \n",
    "    sentences= []\n",
    "    for sent_str in sentences_strings:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "        sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "def get_most_common_words_list(sentences, num_words):\n",
    "    \"\"\" return a list of most common words \n",
    "    \"\"\"\n",
    "    counts_ted_top1000 = []\n",
    "    c = Counter([word for sent in sentences for word in sent])\n",
    "    list_most_common = c.most_common(num_words)\n",
    "    words_most_common = [item[0] for item in list_most_common]\n",
    "    #for word, count in list_most_common:\n",
    "    #    counts_ted_top1000.append(count)\n",
    "    return words_most_common\n",
    "    \n",
    "def replace_unknown_token(sent_list, words_most_common, unknown_token=\"UNK\"):\n",
    "    \"\"\" filter a list of words, replace unknown words with the unknown token \n",
    "    \"\"\"\n",
    "    filtered_list = [word if word in words_most_common else unknown_token for word in sent_list]  # so fast !!!\n",
    "    return filtered_list\n",
    "\n",
    "def tokenize_and_lowercase_most_common(text, words_most_common):\n",
    "    \"\"\" return a list of sentences as lists of words that are most common (in the most common word list)\n",
    "    \"\"\"\n",
    "    text_noparens = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    sentences_strings = []\n",
    "    for line in text_noparens.split('\\n'):\n",
    "        m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "        sentences_strings.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "        \n",
    "    sentences= []\n",
    "    for sent_str in sentences_strings:\n",
    "        tokens = replace_unknown_token(re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split(), words_most_common)\n",
    "        if tokens != []:\n",
    "            sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "def build_dataset(doc_temp, words_most_common):\n",
    "    \"\"\" apply filtering (tokenization, lower-case convertion and common words selection) to each text body in the corpus\n",
    "    \"\"\"\n",
    "    doc_new = [(tokenize_and_lowercase_most_common(doc[0], words_most_common), doc[1]) for doc in doc_temp]\n",
    "    doc_final = [item for item in doc_new if item[0] != []]\n",
    "    return doc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_text = '\\n'.join([train_doc_temp[i][0] for i in range(len(train_doc_temp))])\n",
    "sentences_ted = tokenize_and_lowercase(input_text)\n",
    "# train_doc = [(tokenize_and_lowercase(doc_temp[0]), doc_temp[1]) for doc_temp in train_doc_temp]\n",
    "\n",
    "# Get the list of 9999 most common words \n",
    "words_most_common = get_most_common_words_list(sentences_ted, 9999)\n",
    "\n",
    "# build a temporary training set for Word2Vec embedding\n",
    "train_doc = [(tokenize_and_lowercase_most_common(doc_temp[0], words_most_common), doc_temp[1]) for doc_temp in train_doc_temp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild the vocabulary (add unknown token to the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rebuild_vocab(train_doc):\n",
    "    \"\"\" filtered out unknown tokens, rebuild the dataset with the top vocabulary,\n",
    "        return a list of sentences that can be used to train Word2Vec embedding.\n",
    "    \"\"\"\n",
    "    sentences = [sent for doc in train_doc for sent in doc[0]]\n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = rebuild_vocab(train_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def build_word2vec_model(name, sentences=None, min_count=10, size=100):\n",
    "    \"\"\" train a Word2Vec embedding model or load an existing one \n",
    "    \"\"\"\n",
    "    model = Word2Vec(sentences, min_count=min_count, size=size)\n",
    "    model.save(name)\n",
    "#     if not os.path.isfile(name):\n",
    "#         model = Word2Vec(sentences, min_count=min_count, size=size)\n",
    "#         model.save(name)\n",
    "#     else:\n",
    "#         model = Word2Vec.load(name)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_word2vec_model('word2vec_model', sentences=sentences, min_count=10, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training set \n",
    "train_doc = build_dataset(train_doc_temp, words_most_common)\n",
    "\n",
    "# Validation set \n",
    "valid_doc = build_dataset(valid_doc_temp, words_most_common)\n",
    "\n",
    "# Test set \n",
    "test_doc = build_dataset(test_doc_temp, words_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('corpus_all_9999', train_doc=train_doc, valid_doc=valid_doc, test_doc=test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_text(model, text):\n",
    "    \"\"\" embed the input text as a model vector \n",
    "    \n",
    "    Arguments:\n",
    "        model: Word2Vec model.\n",
    "        text: input text\n",
    "    \n",
    "    Outputs:\n",
    "        embedded vector \n",
    "    \"\"\"\n",
    "    vector_list = [model.wv[word] for sent in text for word in sent]\n",
    "    return sum(vector_list) / len(vector_list)\n",
    "    \n",
    "def embed_corpus(model, corpus):\n",
    "    \"\"\" apply embed_text to each text body in the corpus \n",
    "    \"\"\"\n",
    "    return np.asarray([embed_text(model, doc[0]) for doc in corpus])\n",
    "\n",
    "def encode_label(label, size):\n",
    "    \"\"\" apply one-hot encoding to the label \n",
    "    \"\"\"\n",
    "    l = [0]*size\n",
    "    l[label] = 1\n",
    "    return l\n",
    "\n",
    "def encode_class(corpus, size):\n",
    "    \"\"\" apply encode_label to each sample label in the corpus \n",
    "    \"\"\"\n",
    "    return np.asarray([encode_label(doc[1], size) for doc in corpus])\n",
    "\n",
    "def embedded_with_class(model, doc, size):\n",
    "    doc_x = embed_corpus(model, doc)\n",
    "    doc_y = encode_class(doc, size)\n",
    "    return doc_x, doc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get processed datasets (text embedded and label encoded )\n",
    "\n",
    "train_doc_embed_with_class = embedded_with_class(model, train_doc, len(label_dict))\n",
    "\n",
    "valid_doc_embed_with_class = embedded_with_class(model, valid_doc, len(label_dict))\n",
    "\n",
    "test_doc_embed_with_class = embedded_with_class(model, test_doc, len(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('embedded_corpus_with_labels_all_9999', \n",
    "         train_doc_embed_text=train_doc_embed_with_class[0], train_doc_embed_label=train_doc_embed_with_class[1],\n",
    "         valid_doc_embed_text=valid_doc_embed_with_class[0], valid_doc_embed_label=valid_doc_embed_with_class[1],\n",
    "         test_doc_embed_text=test_doc_embed_with_class[0], test_doc_embed_label=test_doc_embed_with_class[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Means  model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 30000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 50\n",
    "total_batch = int(train_doc_embed_with_class[0].shape[0] / batch_size)\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "y = tf.placeholder(tf.int32, shape=[None, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model trainable parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[100, 256]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[256]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[256, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = tf.tanh(tf.matmul(x, W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(tf.clip_by_value(p, 1e-10, 1.0)), 1))\n",
    "# need to clip the values for stable training experimentally  \n",
    "# loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate model saver to save the model in a session \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(data, index, size):\n",
    "    \"\"\" return next batch in format: index, x batch, y batch\n",
    "    \"\"\"\n",
    "    if index + size <= data[0].shape[0]:\n",
    "        return index+size, data[0][index:index+size], data[1][index:index+size]\n",
    "    else:\n",
    "        return index+size-data[0].shape[0], np.concatenate((data[0][index:],data[0][:index+size-data[0].shape[0]]), 0), \\\n",
    "    np.concatenate((data[1][index:],data[1][:index+size-data[1].shape[0]]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize all trainable model variables \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, Training acc: 0.32, Validation acc: 0.41129 \n",
      "epoch 100, Training acc: 0.66, Validation acc: 0.544355 \n",
      "epoch 200, Training acc: 0.66, Validation acc: 0.560484 \n",
      "epoch 300, Training acc: 0.62, Validation acc: 0.568548 \n",
      "epoch 400, Training acc: 0.58, Validation acc: 0.576613 \n",
      "epoch 500, Training acc: 0.72, Validation acc: 0.572581 \n",
      "epoch 600, Training acc: 0.72, Validation acc: 0.584677 \n",
      "epoch 700, Training acc: 0.66, Validation acc: 0.576613 \n",
      "epoch 800, Training acc: 0.6, Validation acc: 0.564516 \n",
      "epoch 900, Training acc: 0.74, Validation acc: 0.556452 \n",
      "epoch 1000, Training acc: 0.74, Validation acc: 0.556452 \n",
      "epoch 1100, Training acc: 0.7, Validation acc: 0.560484 \n",
      "epoch 1200, Training acc: 0.9, Validation acc: 0.556452 \n",
      "epoch 1300, Training acc: 0.68, Validation acc: 0.552419 \n",
      "epoch 1400, Training acc: 0.72, Validation acc: 0.560484 \n",
      "epoch 1500, Training acc: 0.74, Validation acc: 0.548387 \n",
      "epoch 1600, Training acc: 0.8, Validation acc: 0.544355 \n",
      "epoch 1700, Training acc: 0.8, Validation acc: 0.540323 \n",
      "epoch 1800, Training acc: 0.7, Validation acc: 0.544355 \n",
      "epoch 1900, Training acc: 0.72, Validation acc: 0.548387 \n",
      "epoch 2000, Training acc: 0.82, Validation acc: 0.544355 \n",
      "epoch 2100, Training acc: 0.8, Validation acc: 0.544355 \n",
      "epoch 2200, Training acc: 0.84, Validation acc: 0.544355 \n",
      "epoch 2300, Training acc: 0.8, Validation acc: 0.544355 \n",
      "epoch 2400, Training acc: 0.8, Validation acc: 0.544355 \n",
      "epoch 2500, Training acc: 0.78, Validation acc: 0.544355 \n",
      "epoch 2600, Training acc: 0.72, Validation acc: 0.548387 \n",
      "epoch 2700, Training acc: 0.74, Validation acc: 0.548387 \n",
      "epoch 2800, Training acc: 0.76, Validation acc: 0.556452 \n",
      "epoch 2900, Training acc: 0.78, Validation acc: 0.548387 \n",
      "epoch 3000, Training acc: 0.84, Validation acc: 0.552419 \n",
      "epoch 3100, Training acc: 0.84, Validation acc: 0.544355 \n",
      "epoch 3200, Training acc: 0.86, Validation acc: 0.544355 \n",
      "epoch 3300, Training acc: 0.8, Validation acc: 0.544355 \n",
      "epoch 3400, Training acc: 0.84, Validation acc: 0.552419 \n",
      "epoch 3500, Training acc: 0.82, Validation acc: 0.552419 \n",
      "epoch 3600, Training acc: 0.8, Validation acc: 0.556452 \n",
      "epoch 3700, Training acc: 0.78, Validation acc: 0.552419 \n",
      "epoch 3800, Training acc: 0.78, Validation acc: 0.552419 \n",
      "epoch 3900, Training acc: 0.84, Validation acc: 0.544355 \n",
      "epoch 4000, Training acc: 0.82, Validation acc: 0.540323 \n",
      "epoch 4100, Training acc: 0.84, Validation acc: 0.53629 \n",
      "epoch 4200, Training acc: 0.88, Validation acc: 0.540323 \n",
      "epoch 4300, Training acc: 0.7, Validation acc: 0.544355 \n",
      "epoch 4400, Training acc: 0.82, Validation acc: 0.544355 \n",
      "epoch 4500, Training acc: 0.86, Validation acc: 0.544355 \n",
      "epoch 4600, Training acc: 0.84, Validation acc: 0.544355 \n",
      "epoch 4700, Training acc: 0.72, Validation acc: 0.544355 \n",
      "epoch 4800, Training acc: 0.74, Validation acc: 0.544355 \n",
      "epoch 4900, Training acc: 0.88, Validation acc: 0.544355 \n",
      "epoch 5000, Training acc: 0.8, Validation acc: 0.544355 \n",
      "epoch 5100, Training acc: 0.86, Validation acc: 0.544355 \n",
      "epoch 5200, Training acc: 0.94, Validation acc: 0.540323 \n",
      "epoch 5300, Training acc: 0.7, Validation acc: 0.53629 \n",
      "epoch 5400, Training acc: 0.86, Validation acc: 0.53629 \n",
      "epoch 5500, Training acc: 0.88, Validation acc: 0.53629 \n",
      "epoch 5600, Training acc: 0.82, Validation acc: 0.53629 \n",
      "epoch 5700, Training acc: 0.76, Validation acc: 0.540323 \n",
      "epoch 5800, Training acc: 0.74, Validation acc: 0.53629 \n",
      "epoch 5900, Training acc: 0.84, Validation acc: 0.532258 \n",
      "epoch 6000, Training acc: 0.86, Validation acc: 0.53629 \n",
      "epoch 6100, Training acc: 0.82, Validation acc: 0.53629 \n",
      "epoch 6200, Training acc: 0.88, Validation acc: 0.528226 \n",
      "epoch 6300, Training acc: 0.74, Validation acc: 0.532258 \n",
      "epoch 6400, Training acc: 0.86, Validation acc: 0.532258 \n",
      "epoch 6500, Training acc: 0.84, Validation acc: 0.532258 \n",
      "epoch 6600, Training acc: 0.8, Validation acc: 0.53629 \n",
      "epoch 6700, Training acc: 0.72, Validation acc: 0.532258 \n",
      "epoch 6800, Training acc: 0.82, Validation acc: 0.53629 \n",
      "epoch 6900, Training acc: 0.88, Validation acc: 0.528226 \n",
      "epoch 7000, Training acc: 0.88, Validation acc: 0.524194 \n",
      "epoch 7100, Training acc: 0.8, Validation acc: 0.524194 \n",
      "epoch 7200, Training acc: 0.82, Validation acc: 0.524194 \n",
      "epoch 7300, Training acc: 0.76, Validation acc: 0.528226 \n",
      "epoch 7400, Training acc: 0.88, Validation acc: 0.524194 \n",
      "epoch 7500, Training acc: 0.84, Validation acc: 0.524194 \n",
      "epoch 7600, Training acc: 0.8, Validation acc: 0.524194 \n",
      "epoch 7700, Training acc: 0.7, Validation acc: 0.524194 \n",
      "epoch 7800, Training acc: 0.86, Validation acc: 0.524194 \n",
      "epoch 7900, Training acc: 0.9, Validation acc: 0.524194 \n",
      "epoch 8000, Training acc: 0.88, Validation acc: 0.528226 \n",
      "epoch 8100, Training acc: 0.86, Validation acc: 0.528226 \n",
      "epoch 8200, Training acc: 0.78, Validation acc: 0.528226 \n",
      "epoch 8300, Training acc: 0.74, Validation acc: 0.532258 \n",
      "epoch 8400, Training acc: 0.82, Validation acc: 0.528226 \n",
      "epoch 8500, Training acc: 0.86, Validation acc: 0.532258 \n",
      "epoch 8600, Training acc: 0.8, Validation acc: 0.532258 \n",
      "epoch 8700, Training acc: 0.7, Validation acc: 0.532258 \n",
      "epoch 8800, Training acc: 0.8, Validation acc: 0.532258 \n",
      "epoch 8900, Training acc: 0.84, Validation acc: 0.532258 \n",
      "epoch 9000, Training acc: 0.84, Validation acc: 0.532258 \n",
      "epoch 9100, Training acc: 0.9, Validation acc: 0.532258 \n",
      "epoch 9200, Training acc: 0.72, Validation acc: 0.532258 \n",
      "epoch 9300, Training acc: 0.8, Validation acc: 0.532258 \n",
      "epoch 9400, Training acc: 0.82, Validation acc: 0.532258 \n",
      "epoch 9500, Training acc: 0.8, Validation acc: 0.532258 \n",
      "epoch 9600, Training acc: 0.82, Validation acc: 0.532258 \n",
      "epoch 9700, Training acc: 0.72, Validation acc: 0.532258 \n",
      "epoch 9800, Training acc: 0.72, Validation acc: 0.532258 \n",
      "epoch 9900, Training acc: 0.84, Validation acc: 0.532258 \n",
      "epoch 10000, Training acc: 0.82, Validation acc: 0.532258 \n",
      "epoch 10100, Training acc: 0.86, Validation acc: 0.532258 \n",
      "epoch 10200, Training acc: 0.8, Validation acc: 0.532258 \n",
      "epoch 10300, Training acc: 0.84, Validation acc: 0.53629 \n",
      "epoch 10400, Training acc: 0.78, Validation acc: 0.53629 \n",
      "epoch 10500, Training acc: 0.72, Validation acc: 0.53629 \n",
      "epoch 10600, Training acc: 0.78, Validation acc: 0.508065 \n",
      "epoch 10700, Training acc: 0.92, Validation acc: 0.524194 \n",
      "epoch 10800, Training acc: 0.9, Validation acc: 0.508065 \n",
      "epoch 10900, Training acc: 0.96, Validation acc: 0.504032 \n",
      "epoch 11000, Training acc: 0.94, Validation acc: 0.495968 \n",
      "epoch 11100, Training acc: 1, Validation acc: 0.491935 \n",
      "epoch 11200, Training acc: 0.96, Validation acc: 0.491935 \n",
      "epoch 11300, Training acc: 0.98, Validation acc: 0.508065 \n",
      "epoch 11400, Training acc: 1, Validation acc: 0.508065 \n",
      "epoch 11500, Training acc: 1, Validation acc: 0.520161 \n",
      "epoch 11600, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 11700, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 11800, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 11900, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 12000, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 12100, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 12200, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 12300, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 12400, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 12500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 12600, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 12700, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 12800, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 12900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 13000, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 13100, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 13200, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 13300, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 13400, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 13500, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 13600, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 13700, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 13800, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 13900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 14000, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 14100, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 14200, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 14300, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 14400, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 14500, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 14600, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 14700, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 14800, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 14900, Training acc: 1, Validation acc: 0.556452 \n",
      "epoch 15000, Training acc: 1, Validation acc: 0.556452 \n",
      "epoch 15100, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 15200, Training acc: 1, Validation acc: 0.552419 \n",
      "epoch 15300, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 15400, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 15500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 15600, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 15700, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 15800, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 15900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 16000, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 16100, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 16200, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 16300, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 16400, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 16500, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 16600, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 16700, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 16800, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 16900, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17000, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17100, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17200, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 17300, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 17400, Training acc: 1, Validation acc: 0.548387 \n",
      "epoch 17500, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17600, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17700, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17800, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 17900, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18000, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18100, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18200, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18300, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18400, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 18600, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 18700, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 18800, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 18900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 19000, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 19100, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 19200, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 19300, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 19400, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 19500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 19600, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 19700, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 19800, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 19900, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 20000, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 20100, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 20200, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 20300, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 20400, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 20500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 20600, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 20700, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 20800, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 20900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 21000, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 21100, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 21200, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 21300, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 21400, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 21500, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 21600, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 21700, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 21800, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 21900, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 22000, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 22100, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 22200, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 22300, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 22400, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 22500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 22600, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 22700, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 22800, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 22900, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 23000, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 23100, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 23200, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 23300, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 23400, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 23500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 23600, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 23700, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 23800, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 23900, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 24000, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 24100, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 24200, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 24300, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 24400, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 24500, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 24600, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 24700, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 24800, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 24900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 25000, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 25100, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 25200, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 25300, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 25400, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 25500, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 25600, Training acc: 1, Validation acc: 0.528226 \n",
      "epoch 25700, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 25800, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 25900, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 26000, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 26100, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 26200, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 26300, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 26400, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 26500, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 26600, Training acc: 1, Validation acc: 0.544355 \n",
      "epoch 26700, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 26800, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 26900, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 27000, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 27100, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 27200, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 27300, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 27400, Training acc: 1, Validation acc: 0.540323 \n",
      "epoch 27500, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 27600, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 27700, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 27800, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 27900, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 28000, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 28100, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 28200, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 28300, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 28400, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 28500, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 28600, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 28700, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 28800, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 28900, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29000, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29100, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29200, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29300, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29400, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 29500, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 29600, Training acc: 1, Validation acc: 0.53629 \n",
      "epoch 29700, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29800, Training acc: 1, Validation acc: 0.532258 \n",
      "epoch 29900, Training acc: 1, Validation acc: 0.53629 \n",
      "Model saved in file: models/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# start training \n",
    "for i in range(epoch):\n",
    "    xloss = 0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        index, x_, y_ = next_batch(train_doc_embed_with_class, index, batch_size)\n",
    "        _, xloss, acc_train = sess.run([optimizer, loss, accuracy], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "#         if j % 10 == 0:\n",
    "#             print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 100 == 0:\n",
    "        acc_val = sess.run(accuracy, feed_dict={x:valid_doc_embed_with_class[0], y:valid_doc_embed_with_class[1]})\n",
    "        print(\"epoch %d, Training acc: %g, Validation acc: %g \" % (i, acc_train, acc_val))\n",
    "        \n",
    "save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Model restored.\n",
      "Test acc: 0.544\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"models/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={x:test_doc_embed_with_class[0], y:test_doc_embed_with_class[1]})\n",
    "    print(\"Test acc: %g\" % (acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the learning curves of the model starting from random embeddings, starting from GloVe embeddings (http://nlp.stanford.edu/data/glove.6B.zip; 50 dimensions) or fixed to be the GloVe values. Training in batches is more stable (e.g. 50), which model works best on training vs. test? Which model works best on held-out accuracy?\n",
    "- What happens if you try alternative non-linearities (logistic sigmoid or ReLU instead of tanh)?\n",
    "- What happens if you add dropout to the network?\n",
    "- What happens if you vary the size of the hidden layer?\n",
    "- How would the code change if you wanted to add a second hidden layer?\n",
    "- How does the training algorithm affect the quality of the model?\n",
    "- Project the embeddings of the labels onto 2 dimensions and visualise (each row of the projection matrix V corresponds a label embedding). Do you see anything interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainable parameters \n",
    "W = tf.Variable(tf.truncated_normal(shape=[100, 256]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[256]))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[256, 128]))\n",
    "b2 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[128, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "# additional placeholders \n",
    "dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# model architecture \n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "h2 = tf.nn.relu(tf.matmul(h, W2) + b2)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob=dropout_rate)\n",
    "u = tf.matmul(h2_drop, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "\n",
    "# training preparations \n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(tf.clip_by_value(p, 1e-10, 1.0)), 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, run 0, loss 22.4375\n",
      "epoch 0, run 30, loss 22.5653\n",
      "epoch 0, Test acc: 0.4%\n",
      "epoch 1, run 0, loss 22.5653\n",
      "epoch 1, run 30, loss 21.1838\n",
      "epoch 2, run 0, loss 21.1842\n",
      "epoch 2, run 30, loss 19.3417\n",
      "Model saved in file: models/model1502219553.5451615.ckpt\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    xloss = 0\n",
    "    acc = 0.0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        index, x_, y_ = next_batch(train_doc_embed_with_class, index, batch_size)\n",
    "        _, xloss = sess.run([optimizer, loss], feed_dict={x: x_, y: y_, dropout_rate: 0.5})\n",
    "        \n",
    "        if j % 30 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 100 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict={x:test_doc_embed_with_class[0], y:test_doc_embed_with_class[1], dropout_rate: 1.0})\n",
    "        print(\"epoch %d, Test acc: %g\" % (i, acc * 100), end=\"\")\n",
    "        print(\"%\")\n",
    "        \n",
    "save_path = saver.save(sess, \"models/model\"+str(time.time())+\".ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)  \n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
