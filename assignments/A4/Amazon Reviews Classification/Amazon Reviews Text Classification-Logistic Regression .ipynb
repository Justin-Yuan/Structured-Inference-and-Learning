{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.load('shoes_list_of_review_dicts.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_list = f['reviews_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product/price': 'unknown', 'product/title': \"Caligarius Women's Acheta Pump,Black Calf,6 M\", 'review/helpfulness': '2/2', 'review/summary': 'Beautiful basic pump...', 'review/userId': 'A12O8IHB65BC1S', 'review/text': '... but not enough sizes or colors. Fits true to size on my size 8-1/2 feet.Bottom soles are completely slick... needs some kind of texturing or tread to help prevent slipping.', 'review/score': '4.0', 'product/productId': 'B0009PK7KO', 'review/time': '1169769600', 'review/profileName': 'Fifi'}\n"
     ]
    }
   ],
   "source": [
    "print(reviews_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [review['review/text'] for review in reviews_list]\n",
    "scores = [review['review/score'] for review in reviews_list]\n",
    "labelled_corpus = zip(texts, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389877\n"
     ]
    }
   ],
   "source": [
    "# number of review samples \n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.0', '2.0', '5.0', '3.0', '1.0']\n"
     ]
    }
   ],
   "source": [
    "scores_list = []\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    if scores[i] not in scores_list:\n",
    "        scores_list.append(scores[i])\n",
    "        \n",
    "print(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 5, 5, 5, 3, 5, 5, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "scores = [int(float(score)) for score in scores]\n",
    "print(scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389877\n",
      "[[ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(scores))\n",
    "print(len(labels))\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up the corpus \n",
    "\n",
    "def filter_text_as_sentence_list(text, end=\"END\"):\n",
    "    sentences_list = []\n",
    "    for sent in text.split('.'):\n",
    "        if sent.strip() != '':\n",
    "            sentences_list.append(sent.strip()+\" \"+end)\n",
    "    return sentences_list\n",
    "\n",
    "def filter_text(text, end=\"END\"):\n",
    "    sentences_list = []\n",
    "    for sent in text.split('.'):\n",
    "        if sent.strip() != '':\n",
    "            sentences_list.append(sent.strip()+\" \"+end)\n",
    "    return ' '.join(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but not enough sizes or colors END Fits true to size on my size 8-1/2 feet END Bottom soles are completely slick END needs some kind of texturing or tread to help prevent slipping END\n"
     ]
    }
   ],
   "source": [
    "print(filter_text(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but not en\n"
     ]
    }
   ],
   "source": [
    "sentences_list = filter_text(texts[0])\n",
    "print(sentences_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sentences_list = [filter_text_as_sentence_list(text) for text in texts]\n",
    "all_sentences = [sent for sentence_list in all_sentences_list for sent in sentence_list]\n",
    "all_sentences_as_wordslist = [sent.split() for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389877\n",
      "1696932\n"
     ]
    }
   ],
   "source": [
    "# number of total reviews -> consistency confirmed\n",
    "print(len(all_sentences_list))\n",
    "\n",
    "# number of total sentences -> a lot of short sentences \n",
    "print(len(all_sentences_as_wordslist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def divide_dataset(labelled_doc, num_train, num_valid, num_test=None, shuffle=False):\n",
    "    if num_test == None:\n",
    "        num_test = len(labelled_doc) - num_train - num_valid\n",
    "        \n",
    "    if shuffle:\n",
    "        temp = sample(labelled_doc, len(labelled_doc))\n",
    "    else:\n",
    "        temp = labelled_doc\n",
    "    return temp[:num_train], temp[num_train:-num_test], temp[-num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-9e5a38f41d88>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-9e5a38f41d88>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    train_reviews, valid_reviews, test_reviews = divide_dataset(, 1585, 250, 250, shuffle=True)\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# train_reviews, valid_reviews, test_reviews = divide_dataset(, 1585, 250, 250, shuffle=True)\n",
    "\n",
    "# print(len(train_reviews), len(valid_reviews), len(test_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_common_words_list(sentences, num_words):\n",
    "    # Remember there might be an Unknown token \n",
    "    c = Counter([word for sent in sentences for word in sent])\n",
    "    list_most_common = c.most_common(num_words)\n",
    "    words_most_common = [item[0] for item in list_most_common]\n",
    "    return words_most_common\n",
    "    \n",
    "def replace_unknown_token(sent_list, words_most_common, unknown_token=\"UNK\"):\n",
    "    filtered_list = [word if word in words_most_common else unknown_token for word in sent_list]  # so fast !!!\n",
    "    return filtered_list\n",
    "\n",
    "def map_word_and_index(input_doc, top=10000):\n",
    "    counts_new = []            \n",
    "    words_count_list = Counter([word for doc in input_doc for sent in doc[0] for word in sent]).most_common(top)\n",
    "    word2index = {item[0]: index for index, item in enumerate(words_count_list, 1)}\n",
    "    index2word = {index: item[0] for index, item in enumerate(words_count_list, 1)}\n",
    "    return word2index, index2word\n",
    "\n",
    "def convert2words(doc):\n",
    "    return [word for sent in doc for word in sent]\n",
    "\n",
    "def doc2index(doc, word2index):\n",
    "    return [[word2index[word]] for word in doc]\n",
    "\n",
    "def convert_corpus(corpus, word2index):\n",
    "    corpus_words = [(convert2words(doc[0]), doc[1]) for doc in corpus]\n",
    "    return np.asarray([doc2index(doc[0], word2index) for doc in corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index, index2word = map_word_and_index(train_dc, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding model (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_trained = Word2Vec(all_sentences_as_wordslist, min_count=10, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_trained.save(\"word2vec_model_amazon_review_shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_model_amazon_review_shoes\")\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def build_word2vec_model(name, sentences=None, min_count=10, size=100):\n",
    "    if not os.path.isfile(name):\n",
    "        model = Word2Vec(sentences, min_count=min_count, size=size)\n",
    "        model.save(name)\n",
    "    else:\n",
    "        model = Word2Vec.load(name)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_model = build_word2vec_model('word2vec_model_amazon_review_shoes', sentences=all_sentences_as_wordslist, min_count=10, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_sequence_length = 1000\n",
    "max_num_words = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_texts = [filter_text(text) for text in texts]\n",
    "# print(filtered_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to build index mapping words to their embeddings \n",
    "# embeddings_index[word] = coefficient vector as np.array\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = BASE_DIR + 'glove.6B/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(filtered_texts)\n",
    "sequences = tokenizer.texts_to_sequences(filtered_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 11, 13, 354, 15, 9, 43, 57, 1, 3, 83, 55, 12, 10, 2359, 4, 185, 14, 51, 28, 3, 78, 104, 60, 157, 1, 87, 3, 17, 14, 10, 126, 4, 8756, 1, 7, 11, 26, 29, 4, 65, 37, 109, 43, 79, 1, 3, 17, 683, 14, 32, 2638, 6, 690, 18, 52, 14, 71, 1, 3, 259, 12, 15, 693, 4, 47, 241, 125, 14, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   12   11   13  354   15    9   43   57    1    3   83   55   12   10 2359\n",
      "    4  185   14   51   28    3   78  104   60  157    1   87    3   17   14\n",
      "   10  126    4 8756    1    7   11   26   29    4   65   37  109   43   79\n",
      "    1    3   17  683   14   32 2638    6  690   18   52   14   71    1    3\n",
      "  259   12   15  693    4   47  241  125   14    1]\n"
     ]
    }
   ],
   "source": [
    "print(data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data \n",
    "\n",
    "validation_split = 0.2 \n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix and build the embedding layer \n",
    "\n",
    "num_words = min(max_num_words, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1000)\n",
      "(?, 1000, 100)\n",
      "(?, 100)\n",
      "(?, 6)\n"
     ]
    }
   ],
   "source": [
    "def mm(x):\n",
    "    return tf.reduce_mean(x, axis=1)\n",
    "\n",
    "sequence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "print(sequence_input.shape)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "print(embedded_sequences.shape)\n",
    "x = Lambda(mm)(embedded_sequences)\n",
    "print(x.shape)\n",
    "preds = Dense(6, activation='softmax')(x)\n",
    "print(preds.shape)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,000,606.0\n",
      "Trainable params: 606.0\n",
      "Non-trainable params: 2,000,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 311902 samples, validate on 77975 samples\n",
      "Epoch 1/50\n",
      "311902/311902 [==============================] - 35s - loss: 0.9681 - acc: 0.6594 - val_loss: 0.9638 - val_acc: 0.6609\n",
      "Epoch 2/50\n",
      "311902/311902 [==============================] - 33s - loss: 0.9642 - acc: 0.6598 - val_loss: 0.9600 - val_acc: 0.6605\n",
      "Epoch 3/50\n",
      "311902/311902 [==============================] - 33s - loss: 0.9606 - acc: 0.6597 - val_loss: 0.9568 - val_acc: 0.6613\n",
      "Epoch 4/50\n",
      "311902/311902 [==============================] - 34s - loss: 0.9573 - acc: 0.6599 - val_loss: 0.9537 - val_acc: 0.6616\n",
      "Epoch 5/50\n",
      "311902/311902 [==============================] - 33s - loss: 0.9544 - acc: 0.6604 - val_loss: 0.9508 - val_acc: 0.6617\n",
      "Epoch 6/50\n",
      "311902/311902 [==============================] - 34s - loss: 0.9517 - acc: 0.6607 - val_loss: 0.9483 - val_acc: 0.6622\n",
      "Epoch 7/50\n",
      "311902/311902 [==============================] - 38s - loss: 0.9492 - acc: 0.6609 - val_loss: 0.9460 - val_acc: 0.6613\n",
      "Epoch 8/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9469 - acc: 0.6611 - val_loss: 0.9437 - val_acc: 0.6626\n",
      "Epoch 9/50\n",
      "311902/311902 [==============================] - 43s - loss: 0.9448 - acc: 0.6613 - val_loss: 0.9416 - val_acc: 0.6624\n",
      "Epoch 10/50\n",
      "311902/311902 [==============================] - 43s - loss: 0.9428 - acc: 0.6617 - val_loss: 0.9397 - val_acc: 0.6626\n",
      "Epoch 11/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9409 - acc: 0.6617 - val_loss: 0.9380 - val_acc: 0.6629\n",
      "Epoch 12/50\n",
      "311902/311902 [==============================] - 43s - loss: 0.9392 - acc: 0.6618 - val_loss: 0.9363 - val_acc: 0.6631\n",
      "Epoch 13/50\n",
      "311902/311902 [==============================] - 43s - loss: 0.9375 - acc: 0.6619 - val_loss: 0.9348 - val_acc: 0.6630\n",
      "Epoch 14/50\n",
      "311902/311902 [==============================] - 43s - loss: 0.9360 - acc: 0.6621 - val_loss: 0.9332 - val_acc: 0.6630\n",
      "Epoch 15/50\n",
      "311902/311902 [==============================] - 43s - loss: 0.9346 - acc: 0.6622 - val_loss: 0.9321 - val_acc: 0.6631\n",
      "Epoch 16/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9332 - acc: 0.6623 - val_loss: 0.9307 - val_acc: 0.6637\n",
      "Epoch 17/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9319 - acc: 0.6628 - val_loss: 0.9295 - val_acc: 0.6641\n",
      "Epoch 18/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9306 - acc: 0.6630 - val_loss: 0.9282 - val_acc: 0.6638\n",
      "Epoch 19/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9295 - acc: 0.6633 - val_loss: 0.9271 - val_acc: 0.6645\n",
      "Epoch 20/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9283 - acc: 0.6637 - val_loss: 0.9261 - val_acc: 0.6652\n",
      "Epoch 21/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9272 - acc: 0.6643 - val_loss: 0.9253 - val_acc: 0.6639\n",
      "Epoch 22/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9262 - acc: 0.6647 - val_loss: 0.9240 - val_acc: 0.6649\n",
      "Epoch 23/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9253 - acc: 0.6651 - val_loss: 0.9231 - val_acc: 0.6668\n",
      "Epoch 24/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9243 - acc: 0.6654 - val_loss: 0.9223 - val_acc: 0.6659\n",
      "Epoch 25/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9234 - acc: 0.6659 - val_loss: 0.9214 - val_acc: 0.6662\n",
      "Epoch 26/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9225 - acc: 0.6661 - val_loss: 0.9205 - val_acc: 0.6671\n",
      "Epoch 27/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9216 - acc: 0.6661 - val_loss: 0.9196 - val_acc: 0.6662\n",
      "Epoch 28/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9208 - acc: 0.6663 - val_loss: 0.9190 - val_acc: 0.6665\n",
      "Epoch 29/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9201 - acc: 0.6663 - val_loss: 0.9181 - val_acc: 0.6673\n",
      "Epoch 30/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9193 - acc: 0.6662 - val_loss: 0.9174 - val_acc: 0.6667\n",
      "Epoch 31/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9185 - acc: 0.6665 - val_loss: 0.9172 - val_acc: 0.6663\n",
      "Epoch 32/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9179 - acc: 0.6664 - val_loss: 0.9162 - val_acc: 0.6667\n",
      "Epoch 33/50\n",
      "311902/311902 [==============================] - 41s - loss: 0.9172 - acc: 0.6666 - val_loss: 0.9154 - val_acc: 0.6666\n",
      "Epoch 34/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9165 - acc: 0.6665 - val_loss: 0.9148 - val_acc: 0.6673\n",
      "Epoch 35/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9159 - acc: 0.6667 - val_loss: 0.9144 - val_acc: 0.6671\n",
      "Epoch 36/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9153 - acc: 0.6668 - val_loss: 0.9135 - val_acc: 0.6673\n",
      "Epoch 37/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9147 - acc: 0.6668 - val_loss: 0.9131 - val_acc: 0.6672\n",
      "Epoch 38/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9141 - acc: 0.6669 - val_loss: 0.9125 - val_acc: 0.6678\n",
      "Epoch 39/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9135 - acc: 0.6670 - val_loss: 0.9118 - val_acc: 0.6678\n",
      "Epoch 40/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9130 - acc: 0.6673 - val_loss: 0.9116 - val_acc: 0.6682\n",
      "Epoch 41/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9124 - acc: 0.6673 - val_loss: 0.9109 - val_acc: 0.6682\n",
      "Epoch 42/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9119 - acc: 0.6674 - val_loss: 0.9103 - val_acc: 0.6677\n",
      "Epoch 43/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9114 - acc: 0.6675 - val_loss: 0.9098 - val_acc: 0.6678\n",
      "Epoch 44/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9108 - acc: 0.6675 - val_loss: 0.9093 - val_acc: 0.6678\n",
      "Epoch 45/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9104 - acc: 0.6676 - val_loss: 0.9090 - val_acc: 0.6691\n",
      "Epoch 46/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9099 - acc: 0.6677 - val_loss: 0.9086 - val_acc: 0.6696\n",
      "Epoch 47/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9094 - acc: 0.6679 - val_loss: 0.9080 - val_acc: 0.6683\n",
      "Epoch 48/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9090 - acc: 0.6680 - val_loss: 0.9075 - val_acc: 0.6680\n",
      "Epoch 49/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9086 - acc: 0.6682 - val_loss: 0.9072 - val_acc: 0.6698\n",
      "Epoch 50/50\n",
      "311902/311902 [==============================] - 42s - loss: 0.9081 - acc: 0.6687 - val_loss: 0.9069 - val_acc: 0.6697\n",
      "Training time:  2059.687271595001\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "print(\"Training time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
