{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, TimeDistributed\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Activation, Reshape, merge, Merge\n",
    "from keras.layers import SimpleRNN, GRU, LSTM, Bidirectional\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from Classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordAttentionClassifier(Classifier):\n",
    "    \"\"\" Text classifier with word-level attention \n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100):\n",
    "        super(WordAttentionClassifier, self).__init__(batch_size, epochs, raw_data_path=None, embedded_data_path=embedded_data_path, embedding_dim=embedding_dim) \n",
    "        self.type = 'word_attention'\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" train a hybrid model with Convolution + LSTM\n",
    "        \"\"\"\n",
    "        sentence_input = Input(shape=(self.max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sentence_input)\n",
    "        gru_word = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
    "        dense_word = TimeDistributed((Dense(100)))(gru_word)\n",
    "        tanh_word = TimeDistributed(Activation('tanh'))(dense_word)\n",
    "        att_word = AttLayer()(tanh_word)\n",
    "        preds = Dense(6, activation='softmax')(att_word)\n",
    "        self.model = Model(sentence_input, preds)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\" use RMSprop instead of Adam \n",
    "        \"\"\"\n",
    "        optimizer='rmsprop'\n",
    "        super(WordAttentionClassifier, self).train(optimizer=optimizer)\n",
    "        \n",
    "        \n",
    "class AttLayer(Layer):\n",
    "    \"\"\" Word-level attention layer \n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=None, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[-1], 1),\n",
    "                                     initializer='normal',\n",
    "                                     trainable=True)\n",
    "        #print(self.kernel.get_shape())\n",
    "\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.dot(x, self.kernel)\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/tf.expand_dims(K.sum(ai, axis=1), -1) #ai/K.sum(ai, axis=1).dimshuffle(0, 'x')\n",
    "        \n",
    "        weighted_input = x*weights #tf.expand_dims(weights, -1) #x*weights.dimshuffle(0, 1, 'x')\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\" test the hybird classifier with Convolution + LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_model = WordAttentionClassifier(batch_size=128, epochs=10, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100)\n",
    "    attention_model.build()\n",
    "    attention_model.train()\n",
    "    print(\"constructed Word Attention classifier\")\n",
    "    attention_model.evaluate()\n",
    "    print(\"Word Attention classifier evaluated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
