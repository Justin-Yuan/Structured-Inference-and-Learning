{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import SimpleRNN, GRU, LSTM, Bidirectional\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks are ideal for processing sequential data (in which there is an intrinsic order, for example, a time series). Commonly, a data instance is decomposed into smaller units, each will be processed by the network cell at a time step in a predetermined order. In this way, information from earlier states will be able to accumulate and influence the computation of later states, similar to how human memory works. \n",
    "\n",
    "<img src=\"../images/rnn.png\">\n",
    "\n",
    "Mathematically, RNN can be represented as a cyclic graph, or a recursive formula, in which each new states are computed based on the new inputs and old states. There is also the unrolled version of it as follows. \n",
    "\n",
    "<img src=\"../images/lstm.png\">\n",
    "\n",
    "Each box in the network is a network cell. Typically it is a complex function that takes the previous hidden states and the input at the current time step, and produce a output and a new hidden state. Two major designs for a RNN cell are LSTM and GRU, but there are other variants depending on the task to tackle.\n",
    "\n",
    "<img src=\"../images/lstmcell.png\">\n",
    "\n",
    "A LSTM cell consists of four gates (some may argue for three gates), the essential ones are the input gate, forget gate and output gate. LSTM preserves long term dependencies well, it prevents the exploding gradients problem by implementing new state updates with simple additions instead of matrix multiplications.\n",
    "\n",
    "<img src=\"../images/gru.png\"> \n",
    "\n",
    "A GRU cell only has two gates: a reset gate and a update gate. Essentially it also prevents exploding gradients by gating mechanisms but is more efficient and stable in terms of implementation. It is reported to outperform a LSTM on various tasks like text classification. \n",
    "\n",
    "<img src=\"../images/bidirrnn.png\">\n",
    "\n",
    "A bidirectional RNN goes over the sequece in both forward and backward directions, essentially it is stacking two RNNs to generate states that captures information before and after the current time step. A bidireectional RNN usually outperforms a uni-directional RNN, but is computationally expensive for 2x or above. \n",
    "\n",
    "<img src=\"../images/deeplstm.png\">\n",
    "\n",
    "It is also possible to stack more layers of RNNs to make the model deeper but the computation needed scales up quickly and in practice it seldom goes over two layers in maximum.  \n",
    "\n",
    "a great article & reference to explaining RNN and its variants is here: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNClassifier():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz',\n",
    "                 embedding_dim=100, rnn_type='lstm'):\n",
    "        super(RNNClassifier, self).__init__(batch_size, epochs, raw_data_path=None,\n",
    "                embedded_data_path=embedded_data_path, embedding_dim=embedding_dim) \n",
    "        \n",
    "        # get the RNN model type \n",
    "        self.type = rnn_type\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        if self.type == 'simple':\n",
    "            self.model = self.build_simple_rnn()\n",
    "        elif self.type == 'lstm':\n",
    "            self.model = self.build_lstm()\n",
    "        elif self.type == 'gru':\n",
    "            self.model = self.build_gru()\n",
    "        elif self.type == 'bidirectional':\n",
    "            self.model = self.build_bidirectional_lstm()\n",
    "            \n",
    "    def build_simple_rnn(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        sequence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sequence_input)\n",
    "        x = SimpleRNN(50, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "        preds = Dense(6, activation='softmax')(x)\n",
    "        model_rnn_final_state = Model(sequence_input, preds)\n",
    "        return model_rnn_final_state\n",
    "    \n",
    "    def build_lstm(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        sequence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sequence_input)\n",
    "        x = LSTM(50, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "        preds = Dense(6, activation='softmax')(x)\n",
    "        model_lstm_final_state = Model(sequence_input, preds)\n",
    "        print(model_lstm_final_state)\n",
    "    ï¿¼\n",
    "    def build_gru(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        sequence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sequence_input)\n",
    "        x = GRU(50, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "        preds = Dense(6, activation='softmax')(x)\n",
    "        model_gru_final_state = Model(sequence_input, preds)\n",
    "        return model_gru_final_state\n",
    "    \n",
    "    def build_bidirectional_lstm(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        sequence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sequence_input)\n",
    "        x = Bidirectional(LSTM(50, dropout=0.2, recurrent_dropout=0.2))(embedded_sequences)\n",
    "        preds = Dense(6, activation='softmax')(x)\n",
    "        model_bidirlstm_final_state = Model(sequence_input, preds)\n",
    "        return model_bidirlstm_final_state\n",
    "        \n",
    "    def train(self, loss='categorical_crossentropy', optimizer='adam', model_base_path=\"models/\"):\n",
    "        \"\"\" for Simple RNN, the optimizer needs to implement gradients clipping to prevent explosion \n",
    "        \"\"\"\n",
    "        if self.type == 'simple':\n",
    "            optimizer = optimizers.Adam(clipnorm=1.)\n",
    "        super(RNNClassifier, self).train(optimizer=optimizer, model_base_path=model_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\" test different RNN models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simple RNN with gradient clipping\n",
    "    simple_RNN = RNNClassifier(batch_size=128, epochs=10, raw_data_path=None,\n",
    "                        embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100, rnn_type='simple')\n",
    "    simple_RNN.build()\n",
    "    simple_RNN.train()\n",
    "    print(\"constructed simple RNN classifier\")\n",
    "    simple_RNN.evaluate()\n",
    "    print(\"simple RNN classifier evaluated\")\n",
    "    \n",
    "    # LSTM \n",
    "    lstm = RNNClassifier(batch_size=128, epochs=10, raw_data_path=None,\n",
    "                        embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100, rnn_type='lstm')\n",
    "    lstm.build()\n",
    "    lstm.train()\n",
    "    print(\"constructed LSTM classifier\")\n",
    "    lstm.evaluate()\n",
    "    print(\"LSTM classifier evaluated\")\n",
    "    \n",
    "    # GRU\n",
    "    gru = RNNClassifier(batch_size=128, epochs=10, raw_data_path=None,\n",
    "                        embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100, rnn_type='gru')\n",
    "    gru.build()\n",
    "    gru.train()\n",
    "    print(\"constructed GRU classifier\")\n",
    "    gru.evaluate()\n",
    "    print(\"GRU classifier evaluated\")\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    bidirectional_lstm = RNNClassifier(batch_size=128, epochs=10, raw_data_path=None,\n",
    "                        embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100, rnn_type='bidirectional')\n",
    "    bidirectional_lstm.build()\n",
    "    bidirectional_lstm.train()\n",
    "    print(\"constructed bidirectional LSTM classifier\")\n",
    "    bidirectional_lstm.evaluate()\n",
    "    print(\"bidirectional LSTM classifier evaluated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
