{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time \n",
    "import pickle \n",
    "\n",
    "from sklearn import svm \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from Classifier import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates three baseline classifiers trained on the Amazon Reviews Dataset (Shoes). \n",
    "\n",
    "- MajorityClass always predicts the majority label in the training set. \n",
    "\n",
    "- Multinomial logistic regression takes positional word indices as features and uses softmax to output its predicted labels. \n",
    "\n",
    "- SVM takes processed data (CBOW or Word Embedding) to classify. The word embedding implementation is tested to be working while the CBOW version has not been tested yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MajorityClass(Classifier):\n",
    "    \"\"\" the self.model variable holds the majority class \n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path='data/raw_processed_data.pkl', embedded_data_path='data/data_and_embedding100.npz'):\n",
    "        super(MajorityClass, self).__init__(batch_size, epochs, raw_data_path=raw_data_path, embedded_data_path=embedded_data_path)\n",
    "    \n",
    "    def build_majority_predictor(self):\n",
    "        \"\"\" construct the label distribution dict in training set,\n",
    "            return the majority class as baseline predictor \n",
    "        \"\"\"\n",
    "        max_occr = max(list(self.scores_dict.values()))\n",
    "        for label in self.scores_dict:\n",
    "            if self.scores_dict[label] == max_occr:\n",
    "                self.model = label\n",
    "                \n",
    "        for key in self.scores_dict:\n",
    "            print('class', key, ':', self.scores_dict[key]/self.count)\n",
    "            \n",
    "    def predict_majority_predictor(self, test_data):\n",
    "        \"\"\" for the majority predictor, the model itself is the majority label \n",
    "        \"\"\"\n",
    "        predictions = self.model * np.ones(test_data.shape[0]) \n",
    "        return predictions \n",
    "    \n",
    "    def evaluate_majority_predictor(self, y_test_data=None):\n",
    "        \"\"\" evaluate on the test set by default \n",
    "        \"\"\"\n",
    "        print(\"The majority class is\", self.model)\n",
    "        if y_test_data == None:\n",
    "            preds = np.zeros(shape=(self.y_test.shape[0], self.y_test.shape[1]))\n",
    "            preds[:, self.model] = 1 \n",
    "            acc = np.mean(1*np.equal(np.array(self.y_test), preds))\n",
    "        else:\n",
    "            pred = self.model * np.ones(shape=(y_test_data.shape[0], y_test_data.shape[1]))\n",
    "            acc = np.mean(1*np.equal(np.array(self.y_test_data), preds))\n",
    "        print(\"accuracy is\", acc)\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" save the trained model to the 'models/' directory\n",
    "        \"\"\"\n",
    "        with open('models/majority_class.pkl', 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(Classifier):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100):\n",
    "        super(LogisticRegression, self).__init__(batch_size, epochs, raw_data_path=None, embedded_data_path=embedded_data_path) \n",
    "        \n",
    "        # construct an embedding layer (only necessary for logistic regression)\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embedding_layer = self._construct_embedding_layer()\n",
    "    \n",
    "    def _construct_embedding_layer(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return Embedding(self.num_words,\n",
    "                        self.embedding_dim,\n",
    "                        weights=[self.embedding_matrix],\n",
    "                        input_length=self.max_sequence_length,\n",
    "                        trainable=False)\n",
    "    \n",
    "    def build_logistic_regression(self):\n",
    "        \"\"\" logistic regression without word embedding, each feature is a word index at the specific location \n",
    "        \"\"\"\n",
    "        sequence_input = Input(shape=(self.max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sequence_input)\n",
    "        x = Lambda(self.embedding_mean)(embedded_sequences)\n",
    "        preds = Dense(self.num_labels, activation='softmax')(x)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        #model.summary()\n",
    "        self.model = model\n",
    "    \n",
    "    def train_logistic_regression(self, loss='categorical_crossentropy', optimizer='adam', ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc']) \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.fit(self.x_train, self.y_train,\n",
    "                  batch_size=self.batch_size,\n",
    "                  epochs=self.epochs,\n",
    "                  validation_data=(self.x_val, self.y_val), verbose=0)\n",
    "\n",
    "        print(\"Training time: \", time.time() - start_time)\n",
    "        \n",
    "    def predict_logistic_regression(self, test_data):\n",
    "        \"\"\" feed into the logistic regression model to get predictions \n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(test_data)\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_logistic_regression(self, x_test_data=None, y_test_data=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if x_test_data == None or y_test_data == None:\n",
    "            res = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        else:\n",
    "            res = self.model.evaluate(x_test_data, y_test_data, verbose=0)\n",
    "        print(\"accuracy is %.2f\" % (res[1]*100), end='')  # model.metrics_names[1] is acc\n",
    "        print('%')\n",
    "    \n",
    "    def embedding_mean(self, x):\n",
    "        \"\"\" for logistic regression model \n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(x, axis=1)\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" save the trained model to the 'models/' directory\n",
    "        \"\"\"\n",
    "        self.model.save('models/logistic_regression.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVM(Classifier):\n",
    "    \"\"\" A simple SVM classifier \n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs=50000, raw_data_path=None, \n",
    "                 embedded_data_path='data/data_and_embedding100.npz', model_type='embedding'):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(SVM, self).__init__(batch_size, epochs=epochs, raw_data_path=None, embedded_data_path=embedded_data_path)\n",
    "        self.model_type = model_type   # bow (bag of words) or embedding\n",
    "        \n",
    "        # load or construct dataset for SVM\n",
    "        self._construct_SVM_data()\n",
    "        \n",
    "    def _construct_SVM_data(self):\n",
    "        \"\"\" load the saved data or construct a new one \n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model_type == 'bow':\n",
    "                f = np.load(\"data/svm_bow_data\")\n",
    "                self.x_train_bow = f['x_train_bow']\n",
    "                self.x_val_bow = f['x_val_bow']\n",
    "                self.x_test_bow = f['x_test_bow']\n",
    "            elif self.model_type == 'embedding':\n",
    "                f = np.load(\"data/svm_embedding_data\")\n",
    "                self.x_train_embedded = f['x_train_embedded']\n",
    "                self.x_val_embedded = f['x_val_embedded']\n",
    "                self.x_test_embedded = f['x_test_embedded']\n",
    "                \n",
    "            self.y_train_svm = f['y_train_svm']\n",
    "            self.y_val_svm = f['y_val_svm']\n",
    "            self.y_test_svm = f['y_test_svm']\n",
    "        except:     \n",
    "            if self.model_type == 'bow':\n",
    "                self.x_train_bow = self.convert_doc_feature_vec(self.x_train, self.embedding_matrix)\n",
    "                self.x_val_bow = self.convert_doc_feature_vec(self.x_val, self.embedding_matrix)\n",
    "                self.x_test_bow = self.embed_doc(self.x_test, self.embedding_matrix)\n",
    "            elif self.model_type == 'embedding':\n",
    "                self.x_train_embedded = self.embed_doc(self.x_train, self.embedding_matrix)\n",
    "                self.x_val_embedded = self.embed_doc(self.x_val, self.embedding_matrix)\n",
    "                self.x_test_embedded = self.embed_doc(self.x_test, self.embedding_matrix)\n",
    "\n",
    "            self.y_train_svm = self.convert_labels(self.y_train)\n",
    "            self.y_val_svm = self.convert_labels(self.y_val)\n",
    "            self.y_test_svm = self.convert_labels(self.y_test)\n",
    "            \n",
    "            self._save_SVM_data()\n",
    "        \n",
    "    def _save_SVM_data(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'bow':\n",
    "            np.savez(\"data/svm_bow_data\",\n",
    "                x_train_bow = self.x_train_bow,\n",
    "                x_val_bow = self.x_val_bow,\n",
    "                x_test_bow = self.x_test_bow,\n",
    "                y_train_svm = self.y_train_svm,\n",
    "                y_val_svm = self.y_val_svm,\n",
    "                y_test_svm = self.y_test.svm)\n",
    "        elif self.model_type == 'embedding':\n",
    "            np.savez(\"data/svm_embedding_data\",\n",
    "                x_train_embedded = self.x_train_embedded,\n",
    "                x_val_embedded = self.x_val_embedded,\n",
    "                x_test_embedded = self.x_test_embedded,\n",
    "                y_train_svm = self.y_train_svm,\n",
    "                y_val_svm = self.y_val_svm,\n",
    "                y_test_svm = self.y_test_svm)\n",
    "        \n",
    "    def build_SVM(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.model = svm.LinearSVC(max_iter=self.epochs, verbose=1)\n",
    "        \n",
    "    def train_SVM(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'bow':\n",
    "            self.model.fit(self.x_train_bow, self.y_train_svm)\n",
    "        elif self.model_type == 'embedding':\n",
    "            self.model.fit(self.x_train_embedded, self.y_train_svm)\n",
    "    \n",
    "    def predict_SVM(self, x_test_data):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        preds = self.model.predict(x_test_data)\n",
    "        return preds \n",
    "        \n",
    "    def evaluate_SVM(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'bow':\n",
    "            preds = self.model.predict(self.x_test_bow)\n",
    "            acc = np.mean(1*np.equal(np.array(self.y_test_svm), preds))\n",
    "        elif self.model_type == 'embedding':\n",
    "            preds = self.model.predict(self.x_test_embedded)\n",
    "            acc = np.mean(1*np.equal(np.array(self.y_test_svm, dtype=preds.dtype), preds))\n",
    "        print(\"accuracy: %g\" % (acc*100), end='')\n",
    "        print(\"%\")\n",
    "\n",
    "    # Bag of words \n",
    "    # implementation is flawed, consuming too much memory \n",
    "    def construct_feature_vec(self, text, embedding_matrix):\n",
    "        text_vec = [0] * embedding_matrix.shape[0]\n",
    "        zero_flag = 1\n",
    "        for word in text:\n",
    "            if zero_flag and word < 1:\n",
    "                continue \n",
    "            else:\n",
    "                zero_flag = 0\n",
    "                text_vec[word] += 1\n",
    "        return text_vec \n",
    "\n",
    "    def convert_doc_feature_vec(self, doc, embedding_matrix):\n",
    "        return [self.construct_feature_vec(text, embedding_matrix) for text in doc]\n",
    "    \n",
    "    # Word embedding \n",
    "    def embed_text(self, text, embedding_matrix):\n",
    "        instance_count = 0\n",
    "        text_embedding = np.zeros(embedding_matrix[0].shape)\n",
    "        for word in text:\n",
    "            if word != 0:\n",
    "                instance_count += 1\n",
    "                text_embedding +=  embedding_matrix[word]\n",
    "        return text_embedding/instance_count \n",
    "\n",
    "    def embed_doc(self, doc, embedding_matrix):\n",
    "        return [self.embed_text(text, embedding_matrix) for text in doc]\n",
    "\n",
    "    def convert_labels(self, one_hot_labels):\n",
    "        return [list(label).index(1.0) for label in one_hot_labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the three baseline predictors have similar classification accuracies around 65%-70%, among them the majority class predictor is the lowest as expected since it is not actully learning from data, then comes the multinomial logistic regression and SVM, their test accuracy varies on each new trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw_processed_data.pkl kk\n",
      "data/data_and_embedding100.npz kk\n",
      "loading\n",
      "{4: 63641, 2: 17196, 5: 256503, 3: 28143, 1: 24394}\n",
      "loaded raw processed data\n",
      "5\n",
      "loaded embedded datasets\n",
      "constructed majority class classifier\n",
      "(38987, 6)\n",
      "5\n",
      "accuracy is 0.885568693838\n",
      "majority class preditor evaluated\n",
      "\n",
      "loaded embedded datasets\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_14 (Embedding)     (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 2,000,505\n",
      "Trainable params: 505\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_9 to have shape (None, 5) but got array with shape (315801, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-357879e7b837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlogistic_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/data_and_embedding100.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogistic_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlogistic_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'constructed logitic regression classifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlogistic_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-5a2a72fb5f6f>\u001b[0m in \u001b[0;36mtrain_logistic_regression\u001b[0;34m(self, loss, optimizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                   validation_data=(self.x_val, self.y_val))\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training time: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/justin-y/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1430\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/justin-y/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1310\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1311\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/justin-y/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_9 to have shape (None, 5) but got array with shape (315801, 6)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\" test the baseline classifiers \n",
    "    \"\"\"\n",
    "    \n",
    "    majority_classifier = MajorityClass(batch_size=128, epochs=10, raw_data_path='data/raw_processed_data.pkl')\n",
    "    majority_classifier.build_majority_predictor()\n",
    "    print('constructed majority class classifier')\n",
    "    majority_classifier.evaluate_majority_predictor()\n",
    "    print(\"majority class preditor evaluated\", end='\\n\\n')\n",
    "    \n",
    "    logistic_classifier = LogisticRegression(batch_size=128, epochs=10, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100)\n",
    "    logistic_classifier.build_logistic_regression()\n",
    "    logistic_classifier.train_logistic_regression()\n",
    "    print('constructed logitic regression classifier')\n",
    "    logistic_classifier.evaluate_logistic_regression()\n",
    "    print(\"logistic regression evaluated\")\n",
    "    \n",
    "    svm_classifier = SVM(batch_size=128, epochs=100000, raw_data_path=None, \n",
    "                 embedded_data_path='data/data_and_embedding100.npz', model_type='embedding')\n",
    "    svm_classifier.build_SVM()\n",
    "    svm_classifier.train_SVM()\n",
    "    print('constructed SVM classifier')\n",
    "    svm_classifier.evaluate_SVM()\n",
    "    print(\"SVM with embedding evaluated\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
