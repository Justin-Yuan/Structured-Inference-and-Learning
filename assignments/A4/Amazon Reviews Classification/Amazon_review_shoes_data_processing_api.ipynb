{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "try:\n",
    "    import pickle\n",
    "except:\n",
    "    import cPickle as pickle\n",
    "\n",
    "    \n",
    "\"\"\" investigating Amazon Reviews dataset (shoes)\n",
    "\n",
    "example entry:\n",
    "\n",
    "product/productId: B0009PK7KO\n",
    "product/title: Caligarius Women's Acheta Pump,Black Calf,6 M\n",
    "product/price: unknown\n",
    "review/userId: A12O8IHB65BC1S\n",
    "review/profileName: Fifi\n",
    "review/helpfulness: 2/2\n",
    "review/score: 4.0\n",
    "review/time: 1169769600\n",
    "review/summary: Beautiful basic pump...\n",
    "review/text: ... but not enough sizes or colors. Fits true to size on my size 8-1/2 feet.\n",
    "Bottom soles are completely slick... needs some kind of texturing or tread to help prevent slipping.\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "def build_reviews_list(path):\n",
    "    \"\"\" build a reviews list from the input file, \n",
    "        assuming reviews are separated by '\\n\\n',\n",
    "        get rid of the last empty review if it exits \n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        reviews = f.read()\n",
    "    review_list = reviews.split('\\n\\n')\n",
    "    if review_list[-1] == \"\":\n",
    "        return review_list[:-1]\n",
    "    else:\n",
    "        return review_list\n",
    "\n",
    "def build_review_dict(review):\n",
    "    \"\"\" buuld a review dict from a review entry,\n",
    "        assuming each line has format 'feature_name: feature_content'\n",
    "    \"\"\"\n",
    "    review_dict = {}\n",
    "    feature_list = review.split('\\n')\n",
    "    for feature in feature_list:\n",
    "        feature_and_content = feature.split(': ')\n",
    "        review_dict[feature_and_content[0]] = feature_and_content[1]\n",
    "    return review_dict\n",
    "\n",
    "def build_list_of_review_dict(path):\n",
    "    \"\"\" build a list of review dicts \n",
    "    \"\"\"\n",
    "    review_list = build_reviews_list(path)\n",
    "    return [build_review_dict(review) for review in review_list]\n",
    "    \n",
    "def save_review_dict_list(path, dic_list):\n",
    "    \"\"\" save the list of review dicts with pickle binary encoding\n",
    "    \"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(dic_list, f)\n",
    "    print('data saved to ' + path)\n",
    "\n",
    "def load_review_dic_list(path):\n",
    "    \"\"\" load the data and return \n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sample_score_distribution(scores):\n",
    "    \"\"\" get the frequency distribution of sample scores, \n",
    "        to construct a majority predictor \n",
    "    \"\"\"\n",
    "    scores_dict = {}\n",
    "    count = 0\n",
    "\n",
    "    for score in scores:\n",
    "        count += 1\n",
    "        if score not in scores_dict:\n",
    "            scores_dict[score] = 1\n",
    "        else:\n",
    "            scores_dict[score] += 1\n",
    "     \n",
    "    return scores_dict, count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_and_score_lists(reviews_list):\n",
    "    texts = [review['review/text'] for review in reviews_list]\n",
    "    scores = [int(float(review['review/score'])) for review in reviews_list]\n",
    "    return texts, scores\n",
    "\n",
    "def score_to_categorical(scores):\n",
    "    return to_categorical(np.asarray(scores))\n",
    "\n",
    "\n",
    "# def filter_text_as_sentence_list(text, end=\"END\"):\n",
    "#     \"\"\" replace . with <END> for each sentence in a text entry,\n",
    "#         return list of individual sentences \n",
    "#         @ not useful \n",
    "#     \"\"\"\n",
    "#     sentences_list = []\n",
    "#     for sent in text.split('.'):\n",
    "#         if sent.strip() != '':\n",
    "#             sentences_list.append(sent.strip()+\" \"+end)\n",
    "#     return sentences_list\n",
    "\n",
    "\n",
    "def filter_text(text, end=\"<END>\"):\n",
    "    \"\"\" replace . with <END> for each sentence in a text entry \n",
    "    \"\"\"\n",
    "    sentences_list = []\n",
    "    for sent in text.split('.'):\n",
    "        if sent.strip() != '':\n",
    "            sentences_list.append(sent.strip()+\" \"+end)\n",
    "    return ' '.join(sentences_list)\n",
    "\n",
    "def convert_review_as_sentence(review):\n",
    "    \"\"\" convert one review entry into a list of sentences,\n",
    "        only for hierarchical attention model\n",
    "    \"\"\"\n",
    "    return [sent.strip() for sent in review.split(\" END\") if sent != '' and sent !=' ']\n",
    "\n",
    "def convert_reviews_to_sentences(texts):\n",
    "    \"\"\" return a list of entries, each one is a list of sentences, \n",
    "        only for hierarchical attention model\n",
    "    \"\"\"\n",
    "    return [convert_review_as_sentence(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "max_num_words = 20000      # max number of words to take in a corpus\n",
    "max_sequence_length = 100    # corpus max sentence length 2043\n",
    "embedding_dim = 100\n",
    "\n",
    "\n",
    "# need to build index mapping words to their embeddings \n",
    "# embeddings_index[word] = coefficient vector as np.array\n",
    "\n",
    "def get_embedding_dict(file_name='glove.6B.100d.txt', glove_dir='glove.6B/', base_dir=''):\n",
    "    \"\"\" construct an embedding dict, \n",
    "        each item is a word to its embedding array\n",
    "    \"\"\"\n",
    "    embeddings_index = {}  \n",
    "    with open(os.path.join(base_dir+glove_dir, file_name)) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_tokenizer(filtered_texts, max_num_words):\n",
    "    tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(filtered_texts)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra data processing in hierarchical attention model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_of_sentences(filtered_texts_sentences, tokenizer):\n",
    "    \"\"\" reconstruct into entries where each entry is a list of sentence, each sentence is a list of word indices\n",
    "        This part is only for hierarchical attention model \n",
    "    \"\"\"\n",
    "    return [tokenizer.texts_to_sequences(sent_list) for sent_list in filtered_texts_sentences]\n",
    "\n",
    "def get_sentences_max_num(sequences_sentences):\n",
    "    \"\"\" get the max number of sentences per sample in the corpus, \n",
    "        This part is only for hierarchical attention model \n",
    "    \"\"\"\n",
    "    max_sentence_length = 0\n",
    "    for seq in sequences_sentences:\n",
    "        if len(seq) > max_sentence_length:\n",
    "            max_sentence_length = len(seq)\n",
    "    # print(max_sentence_length)\n",
    "    return max_sentence_length\n",
    "    \n",
    "def pad_sequences_in_list_of_sentence_lists(sequences_sentences):\n",
    "    \"\"\" pad sequences within sentence lists,\n",
    "        This part is only for hierarchical attention model \n",
    "    \"\"\"\n",
    "    return [pad_sequences(index_list, maxlen=max_sequence_length) for index_list in sequences_sentences]\n",
    "\n",
    "def pad_sentences(data_sentences, sentence_max_num, max_length=max_sequence_length):\n",
    "    \"\"\" pad the data in a sentence level,\n",
    "        This part is only for hierarchical attention model \n",
    "    \"\"\"\n",
    "    data_sentences_padded = []\n",
    "    for data_sentence in data_sentences:\n",
    "        if len(data_sentence) < sentence_max_num:\n",
    "            data_sentences_padded.append(np.concatenate(([[0]*max_length]*(max_length-len(data_sentence)), data_sentence), axis=0))\n",
    "    return data_sentences_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end for extra data processing in hierarchical attention model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def split_train_test(data, labels, test_split=0.1):\n",
    "    \"\"\" split the data into a training (included validation) and a test set\n",
    "        x: corpus in indices\n",
    "        y: raw scores as labels (not one-hot encoded yet) \n",
    "    \"\"\"\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    num_test_samples = int(test_split * data.shape[0])\n",
    "\n",
    "    x_train = data[:-num_test_samples]\n",
    "    y_train = labels[:-num_test_samples]\n",
    "    x_test = data[-num_test_samples:]\n",
    "    y_test = labels[-num_test_samples:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def get_embedding_matrix(word_index, embeddings_index, max_num_words=20000, embedding_dim=100):\n",
    "    \"\"\" prepare embedding matrix to build the embedding layer \n",
    "    \"\"\"\n",
    "    num_words = min(max_num_words, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\" by default, Amazon Shoes Reviews data will be processed and saved  \n",
    "    \"\"\"\n",
    "    \n",
    "    max_num_words = 20000      # max number of words to take in a corpus\n",
    "    max_sequence_length = 100    # corpus max sentence length 2043\n",
    "    embedding_dim = 100\n",
    "    \n",
    "    load_path = 'Shoes.txt'\n",
    "    save_path = 'shoes_list_of_review_dicts.pkl'\n",
    "    \n",
    "    reviews_list = build_list_of_review_dict(load_path)\n",
    "    save_review_dict_list(save_path, reviews_list)\n",
    "\n",
    "    # load the data \n",
    "    # data = load_review_dic_list(save_path)\n",
    "        \n",
    "    texts, scores = get_text_and_score_lists(reviews_list)\n",
    "    \n",
    "    # get the majority predictor as baseline\n",
    "    scores_dict, count = get_sample_score_distribution(scores)\n",
    "    \n",
    "    # load embedding index from glove file\n",
    "    embeddings_index = get_embedding_dict(file_name='glove.6B.100d.txt', glove_dir='glove.6B/', base_dir='')\n",
    "\n",
    "    \n",
    "    filtered_texts = [filter_text(text) for text in texts]\n",
    "    labels = scores\n",
    "    # labels = score_to_categorical(scores)   # maybe not now???\n",
    "\n",
    "    # this part of data construction is only for hierarchical attention model\n",
    "    # filtered_texts_sentences = convert_reviews_to_sentences(filtered_texts)\n",
    "    \n",
    "    # indexing and embedding \n",
    "    # get tokenizer for mapping words to indices based on given corpus \n",
    "    tokenizer = get_tokenizer(filtered_texts, max_num_words)\n",
    "    # convert words to indices in each sentence \n",
    "    sequences = tokenizer.texts_to_sequences(filtered_texts)\n",
    "    # get a dict of word: index\n",
    "    word_index = tokenizer.word_index\n",
    "    # pad index sequences with 0s \n",
    "    data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # for hierarchical attention model only\n",
    "    # sequences_sentences = get_sequence_of_sentences(filtered_texts_sentences)\n",
    "    # sentence_max_num = get_sentences_max_num(sequences_sentences)\n",
    "    # data_sentences = pad_sequences_in_list_of_sentence_lists(sequences_sentences)\n",
    "    # data_sentences_padded = pad_sentences(data_sentences, sentence_max_num)\n",
    "\n",
    "    # split the dataset -> train, test\n",
    "    x_train, y_train, x_test, y_test = split_train_test(data, labels)\n",
    "    embedding_matrix = get_embedding_matrix(word_index, embeddings_index)\n",
    "    \n",
    "    # save the processed data \n",
    "    # raw processed data:\n",
    "    processed_data ={\n",
    "        'texts': filtered_texts,\n",
    "        'labels': labels,\n",
    "        'scores_dict':scores_dict,\n",
    "        'count': count, \n",
    "        'embeddings_index': embeddings_index\n",
    "    }\n",
    "    with open('raw_processed_data.pkl', 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "        \n",
    "    # indexed and embedded data\n",
    "    np.savez(\"data_and_embedding100\",\n",
    "             max_num_words=max_num_words\n",
    "             embedding_dim=embedding_dim,\n",
    "             max_sequence_length=max_sequence_length,\n",
    "             x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,\n",
    "             word_index=word_index,\n",
    "             embedding_matrix=embedding_matrix)\n",
    "    \n",
    "    # for hierarchical attention model only\n",
    "    '''\n",
    "    np.savez(\"data_and_embedding100_hierarchical\",\n",
    "             max_num_words=max_num_words\n",
    "             embedding_dim=embedding_dim,\n",
    "             max_sequence_length=max_sequence_length,\n",
    "             x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,\n",
    "             word_index=word_index\n",
    "             embedding_matrix=embedding_matrix,\n",
    "             sentence_max_num=sentence_max_num,\n",
    "             data_sentences_padded=data_sentences_padded)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
