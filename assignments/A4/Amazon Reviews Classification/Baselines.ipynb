{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time \n",
    "import pickle \n",
    "\n",
    "from sklearn import svm \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path=None, embedded_data_path=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # data placeholders\n",
    "        self.x_train = None \n",
    "        self.x_val = None \n",
    "        self.x_test = None \n",
    "        self.y_train = None \n",
    "        self.y_val = None \n",
    "        self.y_test = None \n",
    "        \n",
    "        # load data selectively \n",
    "        if raw_data_path != None:\n",
    "            self._load_raw_data(raw_data_path)\n",
    "        if embedded_data_path != None:\n",
    "            self._load_embedded_data(embedded_data_path)\n",
    "            \n",
    "        # variable to hold the model \n",
    "        self.model = None \n",
    "        \n",
    "    def set_batch_size(self, new_batch_size):\n",
    "        self.batch_size = new_batch_size\n",
    "        \n",
    "    def set_epochs(self, new_epochs):\n",
    "        self.epochs = new_epochs\n",
    "        \n",
    "    def _load_raw_data(self, raw_data_path):\n",
    "        \"\"\" saved data format \n",
    "        processed_data = {\n",
    "            'texts': filtered_texts,\n",
    "            'scores': scores,\n",
    "            'scores_dict':scores_dict,\n",
    "            'count': count, \n",
    "            'embeddings_index': embeddings_index\n",
    "        }\n",
    "        \"\"\"\n",
    "        with open(raw_data_path, 'rb') as f:\n",
    "            raw_data = pickle.load(f)\n",
    "            self.texts = raw_data['texts']\n",
    "            self.scores = raw_data['scores']\n",
    "            self.scores_dict = raw_data['scores_dict']\n",
    "            self.coun = raw_data['count'] \n",
    "            print('loaded raw processed data')\n",
    "        \n",
    "    def _load_embedded_data(self, embedded_data_path, validation_split=0.1):\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "        # f = np.load('data_and_embedding100.npz')\n",
    "        f = np.load(embedded_data_path)\n",
    "        \n",
    "        self.num_labels = int(f['num_labels'])\n",
    "        self.num_words = int(f['num_words'])\n",
    "        self.embedding_dim = int(f['embedding_dim'])\n",
    "        self.max_sequence_length = int(f['max_sequence_length'])\n",
    "\n",
    "        self.x = f['x_train']\n",
    "        self.y = f['y_train']\n",
    "        self.x_test = f['x_test']\n",
    "        self.y_test = f['y_test']\n",
    "\n",
    "        self.embedding_matrix = f['embedding_matrix']\n",
    "        \n",
    "        indices = np.arange(self.x.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices]\n",
    "        self.y = self.y[indices]\n",
    "        num_validation_samples = int(validation_split * self.x.shape[0])\n",
    "\n",
    "        self.x_train = self.x[:-num_validation_samples]\n",
    "        self.y_train = self.y[:-num_validation_samples]\n",
    "        self.x_val = self.x[-num_validation_samples:]\n",
    "        self.y_val = self.y[-num_validation_samples:]\n",
    "        print('loaded embedded datasets')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MajorityClass(Classifier):\n",
    "    \"\"\" the self.model variable holds the majority class \n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path='data/raw_processed_data.pkl'):\n",
    "        super(MajorityClass, self).__init__(batch_size, epochs, raw_data_path=raw_data_path)\n",
    "    \n",
    "    def build_majority_predictor(self):\n",
    "        \"\"\" construct the label distribution dict in training set,\n",
    "            return the majority class as baseline predictor \n",
    "        \"\"\"\n",
    "        max_occr = max(list(self.scores_dict.values()))\n",
    "        for label in self.scores_dict:\n",
    "            if self.scores_dict[label] == max_occr:\n",
    "                self.model = label\n",
    "            \n",
    "    def predict_majority_predictor(self, test_data):\n",
    "        \"\"\" for the majority predictor, the model itself is the majority label \n",
    "        \"\"\"\n",
    "        predictions = self.model * np.ones(test_data.shape[0]) \n",
    "        return predictions \n",
    "    \n",
    "    def evaluate_majority_predictor(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pass \n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" save the trained model to the 'models/' directory\n",
    "        \"\"\"\n",
    "        with open('models/majority_class.pkl', 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(Classifier):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100):\n",
    "        super(LogisticRegression, self).__init__(batch_size, epochs, raw_data_path=None, embedded_data_path=embedded_data_path) \n",
    "        \n",
    "        # construct an embedding layer (only necessary for logistic regression)\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embedding_layer = self._construct_embedding_layer()\n",
    "    \n",
    "    def _construct_embedding_layer(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return Embedding(self.num_words,\n",
    "                        self.embedding_dim,\n",
    "                        weights=[self.embedding_matrix],\n",
    "                        input_length=self.max_sequence_length,\n",
    "                        trainable=False)\n",
    "    \n",
    "    def build_logistic_regression(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        sequence_input = Input(shape=(self.max_sequence_length, ), dtype='int32')\n",
    "        embedded_sequences = self.embedding_layer(sequence_input)\n",
    "        x = Lambda(self.embedding_mean)(embedded_sequences)\n",
    "        preds = Dense(self.num_labels, activation='softmax')(x)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "    \n",
    "    def train_logistic_regression(self, loss='categorical_crossentropy', optimizer='adam', ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc']) \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.fit(self.x_train, self.y_train,\n",
    "                  batch_size=self.batch_size,\n",
    "                  epochs=self.epochs,\n",
    "                  validation_data=(self.x_val, self.y_val))\n",
    "\n",
    "        print(\"Training time: \", time.time() - start_time)\n",
    "        \n",
    "    def predict_logistic_regression(self, test_data):\n",
    "        \"\"\" feed into the logistic regression model to get predictions \n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(test_data)\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_logistic_regression(self, x_test_data, y_test_data):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        res = self.model.evaluate(x_test_data, y_test_data)\n",
    "        print(res[1])  # model.metrics_names[1] is acc \n",
    "    \n",
    "    def embedding_mean(self, x):\n",
    "        \"\"\" for logistic regression model \n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(x, axis=1)\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\" save the trained model to the 'models/' directory\n",
    "        \"\"\"\n",
    "        self.model.save('models/logistic_regression.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVM(Classifier):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, epochs=50000, raw_data_path=None, \n",
    "                 embedded_data_path='data/data_and_embedding100.npz', model_type='embedding'):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(SVM, self).__init__(batch_size, epochs=epochs, raw_data_path=None, embedded_data_path=embedded_data_path)\n",
    "        self.model_type = model_type   # bow (bag of words) or embedding\n",
    "        \n",
    "        # load or construct dataset for SVM\n",
    "        self._construct_SVM_data()\n",
    "        \n",
    "    def _construct_SVM_data(self):\n",
    "        \"\"\" load the saved data or construct a new one \n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model_type == 'bow':\n",
    "                f = np.load(\"data/svm_bow_data\")\n",
    "                self.x_train_bow = f['x_train_bow']\n",
    "                self.x_val_bow = f['x_val_bow']\n",
    "                self.x_test_bow = f['x_test_bow']\n",
    "            elif self.model_type == 'embedding':\n",
    "                f = np.load(\"data/svm_embedding_data\")\n",
    "                self.x_train_embedded = f['x_train_embedded']\n",
    "                self.x_val_embedded = f['x_val_embedded']\n",
    "                self.x_test_embedded = f['x_test_embedded']\n",
    "                \n",
    "            self.y_train_svm = f['y_train_svm']\n",
    "            self.y_val_svm = f['y_val_svm']\n",
    "            self.y_test_svm = f['y_test_svm']\n",
    "        except:     \n",
    "            if self.model_type == 'bow':\n",
    "                self.x_train_bow = self.convert_doc_feature_vec(self.x_train, self.embedding_matrix)\n",
    "                self.x_val_bow = self.convert_doc_feature_vec(self.x_val, self.embedding_matrix)\n",
    "                self.x_test_bow = self.embed_doc(self.x_test, self.embedding_matrix)\n",
    "            elif self.model_type == 'embedding':\n",
    "                self.x_train_embedded = self.embed_doc(self.x_train, self.embedding_matrix)\n",
    "                self.x_val_embedded = self.embed_doc(self.x_val, self.embedding_matrix)\n",
    "                self.x_test_embedded = self.embed_doc(self.x_test, self.embedding_matrix)\n",
    "\n",
    "            self.y_train_svm = self.convert_labels(self.y_train)\n",
    "            self.y_val_svm = self.convert_labels(self.y_val)\n",
    "            self.y_test_svm = self.convert_labels(self.y_test)\n",
    "            \n",
    "            self._save_SVM_data()\n",
    "        \n",
    "    def _save_SVM_data(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'bow':\n",
    "            np.savez(\"data/svm_bow_data\",\n",
    "                x_train_bow = self.x_train_bow,\n",
    "                x_val_bow = self.x_val_bow,\n",
    "                x_test_bow = self.x_test_bow,\n",
    "                y_train_svm = self.y_train_svm,\n",
    "                y_val_svm = self.y_val_svm,\n",
    "                y_test_svm = self.y_test.svm)\n",
    "        elif self.model_type == 'embedding':\n",
    "            np.savez(\"data/svm_embedding_data\",\n",
    "                x_train_embedded = self.x_train_embedded,\n",
    "                x_val_embedded = self.x_val_embedded,\n",
    "                x_test_embedded = self.x_test_embedded,\n",
    "                y_train_svm = self.y_train_svm,\n",
    "                y_val_svm = self.y_val_svm,\n",
    "                y_test_svm = self.y_test_svm)\n",
    "        \n",
    "    def build_SVM(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.model = svm.LinearSVC(max_iter=self.epochs, verbose=1)\n",
    "        \n",
    "    def train_SVM(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'bow':\n",
    "            self.model.fit(self.x_train_bow, self.y_train_svm)\n",
    "        elif self.model_type == 'embedding':\n",
    "            self.model.fit(self.x_train_embedded, self.y_train_svm)\n",
    "    \n",
    "    def predict_SVM(self, x_test_data):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        preds = self.model.predict(x_test_data)\n",
    "        return preds \n",
    "        \n",
    "    def evaluate_SVM(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.model_type == 'bow':\n",
    "            preds = self.model.predict(self.x_test_bow)\n",
    "            acc = np.mean(1*np.equal(np.array(self.y_test_svm), preds))\n",
    "        elif self.model_type == 'embedding':\n",
    "            preds = self.model.predict(self.x_test_embedded)\n",
    "            acc = np.mean(1*np.equal(np.array(self.y_test_svm, dtype=preds.dtype), preds))\n",
    "        print(\"accuracy: %g\" % (acc*100), end='')\n",
    "        print(\"%\")\n",
    "\n",
    "    # Bag of words \n",
    "    # implementation is flawed, consuming too much memory \n",
    "    def construct_feature_vec(self, text, embedding_matrix):\n",
    "        text_vec = [0] * embedding_matrix.shape[0]\n",
    "        zero_flag = 1\n",
    "        for word in text:\n",
    "            if zero_flag and word < 1:\n",
    "                continue \n",
    "            else:\n",
    "                zero_flag = 0\n",
    "                text_vec[word] += 1\n",
    "        return text_vec \n",
    "\n",
    "    def convert_doc_feature_vec(self, doc, embedding_matrix):\n",
    "        return [self.construct_feature_vec(text, embedding_matrix) for text in doc]\n",
    "    \n",
    "    # Word embedding \n",
    "    def embed_text(self, text, embedding_matrix):\n",
    "        instance_count = 0\n",
    "        text_embedding = np.zeros(embedding_matrix[0].shape)\n",
    "        for word in text:\n",
    "            if word != 0:\n",
    "                instance_count += 1\n",
    "                text_embedding +=  embedding_matrix[word]\n",
    "        return text_embedding/instance_count \n",
    "\n",
    "    def embed_doc(self, doc, embedding_matrix):\n",
    "        return [self.embed_text(text, embedding_matrix) for text in doc]\n",
    "\n",
    "    def convert_labels(self, one_hot_labels):\n",
    "        return [list(label).index(1.0) for label in one_hot_labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded raw processed data\n",
      "constructed majority class classifier\n",
      "loaded embedded datasets\n",
      "constructed logitic regression classifier\n",
      "loaded embedded datasets\n",
      "constructed SVM classifier\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\" test the baseline classifiers \n",
    "    \"\"\"\n",
    "    \n",
    "    majority_classifier = MajorityClass(batch_size=128, epochs=10, raw_data_path='data/raw_processed_data.pkl')\n",
    "    print('constructed majority class classifier')\n",
    "    \n",
    "    logistic_classifier = LogisticRegression(batch_size=128, epochs=10, raw_data_path=None, embedded_data_path='data/data_and_embedding100.npz', embedding_dim=100)\n",
    "    print('constructed logitic regression classifier')\n",
    "    \n",
    "    svm_classifier = SVM(batch_size=128, epochs=50000, raw_data_path=None, \n",
    "                 embedded_data_path='data/data_and_embedding100.npz', model_type='embedding')\n",
    "    print('constructed SVM classifier')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
