{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = np.load('shoes_list_of_review_dicts.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_list = f['reviews_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product/productId': 'B0009PK7KO', 'review/time': '1169769600', 'review/profileName': 'Fifi', 'review/text': '... but not enough sizes or colors. Fits true to size on my size 8-1/2 feet.Bottom soles are completely slick... needs some kind of texturing or tread to help prevent slipping.', 'product/title': \"Caligarius Women's Acheta Pump,Black Calf,6 M\", 'product/price': 'unknown', 'review/summary': 'Beautiful basic pump...', 'review/userId': 'A12O8IHB65BC1S', 'review/score': '4.0', 'review/helpfulness': '2/2'}\n"
     ]
    }
   ],
   "source": [
    "print(reviews_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [review['review/text'] for review in reviews_list]\n",
    "scores = [review['review/score'] for review in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scores_list = []\n",
    "\n",
    "# for i in range(len(scores)):\n",
    "#     if scores[i] not in scores_list:\n",
    "#         scores_list.append(scores[i])\n",
    "        \n",
    "# print(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.06256845107559564\n",
      "2.0 0.044106218114943946\n",
      "5.0 0.6579074939019229\n",
      "3.0 0.0721843042805808\n",
      "4.0 0.1632335326269567\n"
     ]
    }
   ],
   "source": [
    "scores_dict = {}\n",
    "count = 0\n",
    "\n",
    "for score in scores:\n",
    "    count += 1\n",
    "    if score not in scores_dict:\n",
    "        scores_dict[score] = 1\n",
    "    else:\n",
    "        scores_dict[score] += 1\n",
    "        \n",
    "for key in scores_dict:\n",
    "    print(key, scores_dict[key]/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up the corpus \n",
    "\n",
    "def filter_text_as_sentence_list(text, end=\"END\"):\n",
    "    sentences_list = []\n",
    "    for sent in text.split('.'):\n",
    "        if sent.strip() != '':\n",
    "            sentences_list.append(sent.strip()+\" \"+end)\n",
    "    return sentences_list\n",
    "\n",
    "def filter_text(text, end=\"END\"):\n",
    "    sentences_list = []\n",
    "    for sent in text.split('.'):\n",
    "        if sent.strip() != '':\n",
    "            sentences_list.append(sent.strip()+\" \"+end)\n",
    "    return ' '.join(sentences_list)\n",
    "\n",
    "def convert_review_as_sentence(review):\n",
    "    return [sent.strip() for sent in review.split(\" END\") if sent != '' and sent !=' ']\n",
    "\n",
    "def convert_reviews_to_sentences(texts):\n",
    "    return [convert_review_as_sentence(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_texts = [filter_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [int(float(score)) for score in scores]\n",
    "labels = to_categorical(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389877\n",
      "but not enough sizes or colors END Fits true to size on my size 8-1/2 feet END Bottom soles are completely slick END needs some kind of texturing or tread to help prevent slipping END\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_texts))\n",
    "print(filtered_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the part of data construction is only for hierarchical attention model\n",
    "filtered_texts_sentences = convert_reviews_to_sentences(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but not enough sizes or colors', 'Fits true to size on my size 8-1/2 feet', 'Bottom soles are completely slick', 'needs some kind of texturing or tread to help prevent slipping']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_texts_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_sequence_length = 100    # corpus max sentence length 2043\n",
    "max_num_words = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to build index mapping words to their embeddings \n",
    "# embeddings_index[word] = coefficient vector as np.array\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = BASE_DIR + 'glove.6B/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(filtered_texts)\n",
    "sequences = tokenizer.texts_to_sequences(filtered_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(sequences[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(word_index['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maxlength = 0\n",
    "# for seq in sequences:\n",
    "#     if len(seq) > maxlength:\n",
    "#         maxlength = len(seq)\n",
    "# print(maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This part is only for hierarchical attention model \n",
    "\n",
    "sequences_sentences = [tokenizer.texts_to_sequences(sent_list) for sent_list in filtered_texts_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 30, 194, 310, 48, 198], [368, 188, 6, 33, 19, 13, 33, 193, 149, 104, 38], [357, 250, 11, 628, 1687], [618, 102, 421, 9, 48, 892, 6, 549, 2080, 1113]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 0\n",
    "for seq in sequences_sentences:\n",
    "    if len(seq) > max_sentence_length:\n",
    "        max_sentence_length = len(seq)\n",
    "print(max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sentences = [pad_sequences(index_list, maxlen=max_sequence_length) for index_list in sequences_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(type(data_sentences[0]))\n",
    "data_sentences_padded = []\n",
    "for data_sentence in data_sentences:\n",
    "    if len(data_sentence) < max_sentence_length:\n",
    "        data_sentences_padded.append(np.concatenate(([[0]*max_sequence_length]*(max_sentence_length-len(data_sentence)), data_sentence), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# print(data_sentences[0][0])\n",
    "print(len(data_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data \n",
    "\n",
    "validation_split = 0.2 \n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix and build the embedding layer \n",
    "\n",
    "num_words = min(max_num_words, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save constructed data and embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saved data for attention model and the others \n",
    "import pickle \n",
    "\n",
    "np.savez(\"data_and_embedding100\", num_words=num_words, embedding_dim=embedding_dim,\n",
    "                             max_sequence_length=max_sequence_length, data=data,\n",
    "                             labels=labels, embedding_matrix=embedding_matrix,\n",
    "                            max_sentence_length=max_sentence_length,\n",
    "                            sequences_sentences=sequences_sentences)\n",
    "\n",
    "with open('word2index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
