{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_num = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load('corpus_all_47001.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_doc', 'valid_doc', 'test_doc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = f['train_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_doc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_doc = f['valid_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = f['test_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "min_count = 100\n",
    "max_count = 0\n",
    "\n",
    "for doc in train_doc:\n",
    "    for sent in doc[0]:\n",
    "        if len(sent) < min_count:\n",
    "            min_count = len(sent)\n",
    "        if len(sent) > max_count:\n",
    "            max_count = len(sent)\n",
    "            \n",
    "print(min_count)\n",
    "print(max_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert2words(doc):\n",
    "    return [word for sent in doc for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_doc_words = np.asarray([(convert2words(doc[0]), doc[1]) for doc in train_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_doc_words[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_new = []\n",
    "            \n",
    "words_count_list = Counter([word for doc in train_doc for sent in doc[0] for word in sent]).most_common(47001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47001\n",
      "[('the', 148314), ('and', 106841), ('to', 90699), ('of', 81748), ('a', 75296), ('that', 67989), ('i', 58165), ('in', 56239), ('it', 51467), ('we', 49525)]\n"
     ]
    }
   ],
   "source": [
    "print(len(words_count_list))\n",
    "print(words_count_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(2, 2)\n",
      "(3, 3)\n",
      "(4, 4)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,4]\n",
    "for i in enumerate(l, 1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {item[0]: index for index, item in enumerate(words_count_list, 1)}\n",
    "index2word = {index: item[0] for index, item in enumerate(words_count_list, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "print(word2index['and'])\n",
    "print(index2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2index(doc, word2index):\n",
    "    return [word2index[word] for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wordasindex = np.asarray([doc2index(doc[0], word2index) for doc in train_doc_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1579,)\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(train_wordasindex.shape)\n",
    "print(train_wordasindex[0][0])\n",
    "# print(train_wordasindex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API for corpuse format conversion \n",
    "\n",
    "def convert2words(doc):\n",
    "    return [word for sent in doc for word in sent]\n",
    "\n",
    "def doc2index(doc, word2index):\n",
    "    return [[word2index[word]] for word in doc]\n",
    "\n",
    "def convert_corpus(corpus, word2index):\n",
    "    corpus_words = [(convert2words(doc[0]), doc[1]) for doc in corpus]\n",
    "    return np.asarray([doc2index(doc[0], word2index) for doc in corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wordasindex = convert_corpus(train_doc, word2index)\n",
    "valid_wordasindex = convert_corpus(valid_doc, word2index)\n",
    "test_wordasindex = convert_corpus(test_doc, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1579,)\n",
      "1364\n",
      "[72]\n"
     ]
    }
   ],
   "source": [
    "print(train_wordasindex.shape)\n",
    "print(len(train_wordasindex[0]))\n",
    "print(train_wordasindex[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_model_vocab47001_mincount1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47001\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "child\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(word2index['computer'])\n",
    "print(index2word[394])\n",
    "# print(model.wv[index2word[394]])\n",
    "print(model.wv[index2word[394]].shape)\n",
    "# model.most_similar(index2word[394])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_from_index(model, idx):\n",
    "    if idx != 0:\n",
    "        return model.wv[index2word[idx]]\n",
    "    else:\n",
    "        return np.zeros(shape=model.wv[index2word[1]].shape)\n",
    "\n",
    "def embed_corpus(corpus, word2index, model):\n",
    "    temp = list(corpus)\n",
    "    return np.asarray([[embed_from_index(model, idx) for sent in doc for idx in sent] for doc in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_wordasindex_embed = embed_corpus(train_doc, word2index, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_wordasindex_embed.shape)\n",
    "# print(len(train_wordasindex_embed[0]))\n",
    "# print(train_wordasindex_embed[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_wordasindex_embed[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_wordasindex_embed = embed_corpus(train_doc, word2index, model)\n",
    "# valid_wordasindex_embed = embed_corpus(valid_doc, word2index, model)\n",
    "# test_wordasindex_embed = embed_corpus(test_doc, word2index, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_label(label, size):\n",
    "    l = [0]*size\n",
    "    l[label] = 1\n",
    "    return l\n",
    "\n",
    "def encode_class(corpus, size):\n",
    "    return np.asarray([encode_label(doc[1], size) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = encode_class(train_doc, 8)\n",
    "valid_label = encode_class(valid_doc, 8)\n",
    "test_label = encode_class(test_doc, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1579, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 966\n",
      "1 275\n",
      "2 97\n",
      "3 112\n",
      "4 18\n",
      "5 85\n",
      "6 10\n",
      "7 16\n"
     ]
    }
   ],
   "source": [
    "label_tally = {i:0 for i in range(8)}\n",
    "\n",
    "for doc in train_doc:\n",
    "    label_tally[doc[1]] += 1\n",
    "    \n",
    "for key in label_tally:\n",
    "    print(key, label_tally[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = (train_wordasindex, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1579,)\n",
      "(1579, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].shape)\n",
    "print(train_data[1].shape)\n",
    "# print(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "6686\n"
     ]
    }
   ],
   "source": [
    "min_count = 100\n",
    "max_count = 0\n",
    "\n",
    "for doc in valid_wordasindex:\n",
    "    if len(doc) < min_count:\n",
    "        min_count = len(doc)\n",
    "    if len(doc) > max_count:\n",
    "        max_count = len(doc)\n",
    "\n",
    "print(min_count)\n",
    "print(max_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 47001\n",
    "\n",
    "learning_rate = 0.001\n",
    "maxlen = 6700\n",
    "batch_size = 50\n",
    "total_batch = int(train_data[0].shape[0]/batch_size)\n",
    "input_dims = 100\n",
    "num_hidden= 50\n",
    "epochs = 100\n",
    "\n",
    "index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(total_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1579,)\n"
     ]
    }
   ],
   "source": [
    "train_data\n",
    "train_wordasindex\n",
    "valid_wordasindex\n",
    "test_wordasindex\n",
    "train_label\n",
    "valid_label\n",
    "test_label\n",
    "print(train_wordasindex.shape)\n",
    "# print(train_wordasindex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in train_wordasindex:\n",
    "    for word_index in sent:\n",
    "        if word_index == 0:\n",
    "            print(\"failed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0], [0], [0], [0]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[0]]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_with_zeros(sequence, maxlen):\n",
    "    if len(sequence) > maxlen:\n",
    "        raise Exception\n",
    "    else:\n",
    "        return sequence+[[0]]*(maxlen-len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, [0], [0]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = [1, 2, 3]\n",
    "pad_with_zeros(k,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_pad_with_zeros(corpus, maxlen):\n",
    "    return np.asarray([pad_with_zeros(sent, maxlen) for sent in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = corpus_pad_with_zeros(train_wordasindex, maxlen)\n",
    "valid_corpus = corpus_pad_with_zeros(valid_wordasindex, maxlen)\n",
    "# test_corpus = corpus_pad_with_zeros(test_wordasindex, maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_corpus.shape)\n",
    "# print(train_label.shape)\n",
    "# print(train_corpus[0].shape)\n",
    "# print(train_corpus[3].shape)\n",
    "# print(train_corpus[45].shape)\n",
    "# print(train_corpus[90].shape)\n",
    "# print(train_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_embed = embed_corpus(train_corpus, word2index, model)\n",
    "valid_corpus_embed = embed_corpus(valid_corpus, word2index, model)\n",
    "# test_corpus_embed = embed_corpus(test_corpus, word2index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_corpus = corpus_pad_with_zeros(test_wordasindex, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_corpus_embed = embed_corpus(test_corpus, word2index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('corpus_embeded_all_47001', train_corpus_embed=train_corpus_embed, valid_corpus_embed=valid_corpus_embed, test_corpus_embed=test_corpus_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_corpus_embed.shape)\n",
    "# print(train_label.shape)\n",
    "# print(train_corpus_embed[0].shape)\n",
    "# print(train_corpus_embed[3].shape)\n",
    "# print(train_corpus_embed[45].shape)\n",
    "# print(train_corpus_embed[90].shape)\n",
    "# print(train_corpus_embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 6700, 1)\n",
      "(250, 8)\n"
     ]
    }
   ],
   "source": [
    "print(valid_corpus.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data, index, size):\n",
    "    \"\"\" return next batch in format: index, x batch, y batch\n",
    "    \"\"\"\n",
    "    if index + size <= data[0].shape[0]:\n",
    "        return index+size, data[0][index:index+size], data[1][index:index+size]\n",
    "    else:\n",
    "        return index+size-data[0].shape[0], np.concatenate((data[0][index:],data[0][:index+size-data[0].shape[0]]), 0), \\\n",
    "    np.concatenate((data[1][index:],data[1][:index+size-data[1].shape[0]]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow wants a data format of [Batch Size, Sequence Length, Input Dimension].\n",
    "x = tf.placeholder(tf.float32, [None, maxlen, input_dims])\n",
    "y = tf.placeholder(tf.float32, [None, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('cellsdef'):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('lstm_rnn_k'):\n",
    "    val, state = tf.nn.dynamic_rnn(cell_fw, x, dtype=tf.float32, sequence_length=length(x))    # val are the h_ts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6700, 50)\n",
      "2\n",
      "(?, 50)\n",
      "(?, 50)\n"
     ]
    }
   ],
   "source": [
    "print(val.shape)\n",
    "print(len(state))\n",
    "print(state[0].shape)\n",
    "print(state[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast:0\", shape=(?,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal(shape=[50, 32]))\n",
    "b = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "\n",
    "# W2 = tf.Variable(tf.truncated_normal(shape=[32, 16]))\n",
    "# b2 = tf.Variable(tf.constant(0.0, shape=[16]))\n",
    "\n",
    "\n",
    "V = tf.Variable(tf.truncated_normal(shape=[32, 8]))\n",
    "c = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "\n",
    "dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "h = tf.nn.elu(tf.matmul(tf.reduce_mean(val, axis=1), W) + b)\n",
    "# h_drop = tf.nn.dropout(h, keep_prob=dropout_rate)\n",
    "# h2 = tf.nn.elu(tf.matmul(h_drop, W2) + b2)\n",
    "\n",
    "# h = tf.nn.relu(tf.matmul(state[1], W) + b)\n",
    "u = tf.matmul(h, V) + c\n",
    "p = tf.nn.softmax(u)\n",
    "pred = tf.argmax(p, 1)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-tf.cast(y, tf.float32)*tf.log(p), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8)\n",
      "(?, 3) (?, 3)\n",
      "(?,)\n",
      "3\n",
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "# using HK@k??\n",
    "print(p.shape)\n",
    "a, b = tf.nn.top_k(p, k=3)\n",
    "print(a.shape, b.shape)\n",
    "print(tf.argmax(y, 1).shape)\n",
    "t = tf.unstack(b, axis=1)\n",
    "print(len(t))\n",
    "print(t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = tf.nn.top_k(p, k=3)\n",
    "y_truth = tf.cast(tf.argmax(y, 1), tf.int32)\n",
    "top_list = tf.unstack(indices, axis=1)\n",
    "hit_rate = tf.reduce_mean(tf.cast(tf.equal(top_list[0], y_truth), tf.float32)\\\n",
    "    + tf.cast(tf.equal(top_list[1], y_truth), tf.float32)\\\n",
    "    + tf.cast(tf.equal(top_list[2], y_truth), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, tf.argmax(y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_most_likely_class = tf.reduce_mean(tf.cast(tf.equal(pred, tf.cast(comp, tf.int64)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n"
     ]
    }
   ],
   "source": [
    "most_class = np.zeros([250])\n",
    "print(most_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, run 0, loss 0.663523\n",
      "epoch 0, run 10, loss 0.749787\n",
      "epoch 0, run 20, loss 1.28233\n",
      "epoch 0, run 30, loss 0.527809\n",
      "epoch 0, Validation acc: 45.2%, hit rate: 78.4%, class 0 accuracy: 78.4%\n",
      "epoch 1, run 0, loss 0.759069\n",
      "epoch 1, run 10, loss 0.755754\n",
      "epoch 1, run 20, loss 1.25488\n",
      "epoch 1, run 30, loss 0.488679\n",
      "epoch 1, Validation acc: 46%, hit rate: 78.8%, class 0 accuracy: 72.4%\n",
      "epoch 2, run 0, loss 0.657462\n",
      "epoch 2, run 10, loss 1.01113\n",
      "epoch 2, run 20, loss 1.13904\n",
      "epoch 2, run 30, loss 0.566496\n",
      "epoch 2, Validation acc: 46.4%, hit rate: 79.6%, class 0 accuracy: 74%\n",
      "epoch 3, run 0, loss 0.530997\n",
      "epoch 3, run 10, loss 0.722613\n",
      "epoch 3, run 20, loss 1.04754\n",
      "epoch 3, run 30, loss 0.881299\n",
      "epoch 3, Validation acc: 46.8%, hit rate: 78.8%, class 0 accuracy: 69.6%\n",
      "epoch 4, run 0, loss 0.530286\n",
      "epoch 4, run 10, loss 0.720508\n",
      "epoch 4, run 20, loss 1.20403\n",
      "epoch 4, run 30, loss 0.943459\n",
      "epoch 4, Validation acc: 46%, hit rate: 78%, class 0 accuracy: 74%\n",
      "epoch 5, run 0, loss 0.600844\n",
      "epoch 5, run 10, loss 0.746224\n",
      "epoch 5, run 20, loss 1.25136\n",
      "epoch 5, run 30, loss 0.863448\n",
      "epoch 5, Validation acc: 46.8%, hit rate: 80%, class 0 accuracy: 70%\n",
      "epoch 6, run 0, loss 0.826843\n",
      "epoch 6, run 10, loss 0.943506\n",
      "epoch 6, run 20, loss 1.14864\n",
      "epoch 6, run 30, loss 0.918161\n",
      "epoch 6, Validation acc: 46%, hit rate: 78.8%, class 0 accuracy: 67.2%\n",
      "epoch 7, run 0, loss 0.932288\n",
      "epoch 7, run 10, loss 0.772262\n",
      "epoch 7, run 20, loss 0.876506\n",
      "epoch 7, run 30, loss 1.29921\n",
      "epoch 7, Validation acc: 45.2%, hit rate: 81.2%, class 0 accuracy: 61.6%\n",
      "epoch 8, run 0, loss 0.794738\n",
      "epoch 8, run 10, loss 0.698013\n",
      "epoch 8, run 20, loss 0.576075\n",
      "epoch 8, run 30, loss 1.10827\n",
      "epoch 8, Validation acc: 43.2%, hit rate: 79.2%, class 0 accuracy: 62.4%\n",
      "epoch 9, run 0, loss 0.968915\n",
      "epoch 9, run 10, loss 0.510292\n",
      "epoch 9, run 20, loss 0.843663\n",
      "epoch 9, run 30, loss 1.14182\n",
      "epoch 9, Validation acc: 44.8%, hit rate: 78.8%, class 0 accuracy: 60.4%\n",
      "epoch 10, run 0, loss 1.31872\n",
      "epoch 10, run 10, loss 0.480491\n",
      "epoch 10, run 20, loss 0.95229\n",
      "epoch 10, run 30, loss 1.31183\n",
      "epoch 10, Validation acc: 44%, hit rate: 80.8%, class 0 accuracy: 64.8%\n",
      "epoch 11, run 0, loss 1.00723\n",
      "epoch 11, run 10, loss 0.394291\n",
      "epoch 11, run 20, loss 0.872498\n",
      "epoch 11, run 30, loss 1.00486\n",
      "epoch 11, Validation acc: 43.6%, hit rate: 80.4%, class 0 accuracy: 70.8%\n",
      "epoch 12, run 0, loss 1.15331\n",
      "epoch 12, run 10, loss 0.555944\n",
      "epoch 12, run 20, loss 0.977247\n",
      "epoch 12, run 30, loss 0.827854\n",
      "epoch 12, Validation acc: 43.2%, hit rate: 80.4%, class 0 accuracy: 73.6%\n",
      "epoch 13, run 0, loss 1.0707\n",
      "epoch 13, run 10, loss 0.537446\n",
      "epoch 13, run 20, loss 0.844851\n",
      "epoch 13, run 30, loss 0.960313\n",
      "epoch 13, Validation acc: 44%, hit rate: 82%, class 0 accuracy: 54.4%\n",
      "epoch 14, run 0, loss 1.03292\n",
      "epoch 14, run 10, loss 0.450078\n",
      "epoch 14, run 20, loss 0.651667\n",
      "epoch 14, run 30, loss 0.937186\n",
      "epoch 14, Validation acc: 43.2%, hit rate: 80.8%, class 0 accuracy: 46.8%\n",
      "epoch 15, run 0, loss 1.14607\n",
      "epoch 15, run 10, loss 0.479329\n",
      "epoch 15, run 20, loss 0.596439\n",
      "epoch 15, run 30, loss 0.791562\n",
      "epoch 15, Validation acc: 46%, hit rate: 80.4%, class 0 accuracy: 46.8%\n",
      "epoch 16, run 0, loss 1.00447\n",
      "epoch 16, run 10, loss 0.464563\n",
      "epoch 16, run 20, loss 0.650882\n",
      "epoch 16, run 30, loss 1.10449\n",
      "epoch 16, Validation acc: 44%, hit rate: 80.8%, class 0 accuracy: 61.2%\n",
      "epoch 17, run 0, loss 0.872292\n",
      "epoch 17, run 10, loss 0.366932\n",
      "epoch 17, run 20, loss 0.561955\n",
      "epoch 17, run 30, loss 1.03949\n",
      "epoch 17, Validation acc: 44.4%, hit rate: 80%, class 0 accuracy: 67.2%\n",
      "epoch 18, run 0, loss 0.849507\n",
      "epoch 18, run 10, loss 0.591434\n",
      "epoch 18, run 20, loss 0.733025\n",
      "epoch 18, run 30, loss 0.753264\n",
      "epoch 18, Validation acc: 42.8%, hit rate: 78.8%, class 0 accuracy: 72.8%\n",
      "epoch 19, run 0, loss 1.08327\n",
      "epoch 19, run 10, loss 0.498381\n",
      "epoch 19, run 20, loss 0.709427\n",
      "epoch 19, run 30, loss 0.857735\n",
      "epoch 19, Validation acc: 43.2%, hit rate: 77.6%, class 0 accuracy: 74%\n",
      "epoch 20, run 0, loss 0.929171\n",
      "epoch 20, run 10, loss 0.333693\n",
      "epoch 20, run 20, loss 0.693254\n",
      "epoch 20, run 30, loss 0.737641\n",
      "epoch 20, Validation acc: 44.4%, hit rate: 78.4%, class 0 accuracy: 74%\n",
      "epoch 21, run 0, loss 0.699428\n",
      "epoch 21, run 10, loss 0.286632\n",
      "epoch 21, run 20, loss 0.520236\n",
      "epoch 21, run 30, loss 0.940391\n",
      "epoch 21, Validation acc: 45.6%, hit rate: 75.2%, class 0 accuracy: 65.6%\n",
      "epoch 22, run 0, loss 0.712709\n",
      "epoch 22, run 10, loss 0.360145\n",
      "epoch 22, run 20, loss 0.71837\n",
      "epoch 22, run 30, loss 1.2033\n",
      "epoch 22, Validation acc: 42.8%, hit rate: 78.8%, class 0 accuracy: 67.6%\n",
      "epoch 23, run 0, loss 1.085\n",
      "epoch 23, run 10, loss 0.832573\n",
      "epoch 23, run 20, loss 0.780066\n",
      "epoch 23, run 30, loss 1.14035\n",
      "epoch 23, Validation acc: 44%, hit rate: 78.4%, class 0 accuracy: 72%\n",
      "epoch 24, run 0, loss 1.05047\n",
      "epoch 24, run 10, loss 0.694072\n",
      "epoch 24, run 20, loss 0.827411\n",
      "epoch 24, run 30, loss 0.943797\n",
      "epoch 24, Validation acc: 42%, hit rate: 77.6%, class 0 accuracy: 76.8%\n",
      "epoch 25, run 0, loss 1.02702\n",
      "epoch 25, run 10, loss 0.576852\n",
      "epoch 25, run 20, loss 0.561109\n",
      "epoch 25, run 30, loss 0.421177\n",
      "epoch 25, Validation acc: 46%, hit rate: 77.2%, class 0 accuracy: 68%\n",
      "epoch 26, run 0, loss 0.932771\n",
      "epoch 26, run 10, loss 0.746776\n",
      "epoch 26, run 20, loss 0.476857\n",
      "epoch 26, run 30, loss 0.622146\n",
      "epoch 26, Validation acc: 44.8%, hit rate: 76.4%, class 0 accuracy: 74.4%\n",
      "epoch 27, run 0, loss 0.719301\n",
      "epoch 27, run 10, loss 1.23352\n",
      "epoch 27, run 20, loss 0.438452\n",
      "epoch 27, run 30, loss 0.692322\n",
      "epoch 27, Validation acc: 44.8%, hit rate: 77.6%, class 0 accuracy: 58.4%\n",
      "epoch 28, run 0, loss 0.46475\n",
      "epoch 28, run 10, loss 0.981322\n",
      "epoch 28, run 20, loss 0.316573\n",
      "epoch 28, run 30, loss 0.724109\n",
      "epoch 28, Validation acc: 44.4%, hit rate: 79.6%, class 0 accuracy: 60.4%\n",
      "epoch 29, run 0, loss 0.550425\n",
      "epoch 29, run 10, loss 0.914013\n",
      "epoch 29, run 20, loss 0.340312\n",
      "epoch 29, run 30, loss 0.727524\n",
      "epoch 29, Validation acc: 46.8%, hit rate: 80.8%, class 0 accuracy: 58.8%\n",
      "epoch 30, run 0, loss 0.751351\n",
      "epoch 30, run 10, loss 1.02164\n",
      "epoch 30, run 20, loss 0.379623\n",
      "epoch 30, run 30, loss 0.731497\n",
      "epoch 30, Validation acc: 45.6%, hit rate: 78%, class 0 accuracy: 56.4%\n",
      "epoch 31, run 0, loss 0.669747\n",
      "epoch 31, run 10, loss 0.787025\n",
      "epoch 31, run 20, loss 0.318281\n",
      "epoch 31, run 30, loss 0.523843\n",
      "epoch 31, Validation acc: 46.4%, hit rate: 77.6%, class 0 accuracy: 60%\n",
      "epoch 32, run 0, loss 0.641572\n",
      "epoch 32, run 10, loss 0.668019\n",
      "epoch 32, run 20, loss 0.284246\n",
      "epoch 32, run 30, loss 0.463529\n",
      "epoch 32, Validation acc: 47.6%, hit rate: 80%, class 0 accuracy: 57.6%\n",
      "epoch 33, run 0, loss 0.599145\n",
      "epoch 33, run 10, loss 0.702336\n",
      "epoch 33, run 20, loss 0.23182\n",
      "epoch 33, run 30, loss 0.535479\n",
      "epoch 33, Validation acc: 44%, hit rate: 76%, class 0 accuracy: 65.2%\n",
      "epoch 34, run 0, loss 0.396863\n",
      "epoch 34, run 10, loss 0.878336\n",
      "epoch 34, run 20, loss 0.340286\n",
      "epoch 34, run 30, loss 0.514165\n",
      "epoch 34, Validation acc: 43.6%, hit rate: 77.2%, class 0 accuracy: 71.2%\n",
      "epoch 35, run 0, loss 0.48473\n",
      "epoch 35, run 10, loss 0.749008\n",
      "epoch 35, run 20, loss 0.483275\n",
      "epoch 35, run 30, loss 0.359063\n",
      "epoch 35, Validation acc: 42.4%, hit rate: 75.6%, class 0 accuracy: 66%\n",
      "epoch 36, run 0, loss 0.488719\n",
      "epoch 36, run 10, loss 0.757046\n",
      "epoch 36, run 20, loss 0.384078\n",
      "epoch 36, run 30, loss 0.558725\n",
      "epoch 36, Validation acc: 45.6%, hit rate: 79.2%, class 0 accuracy: 58.4%\n",
      "epoch 37, run 0, loss 0.435575\n",
      "epoch 37, run 10, loss 0.770143\n",
      "epoch 37, run 20, loss 0.293292\n",
      "epoch 37, run 30, loss 0.525615\n",
      "epoch 37, Validation acc: 46%, hit rate: 75.6%, class 0 accuracy: 64.8%\n",
      "epoch 38, run 0, loss 0.523959\n",
      "epoch 38, run 10, loss 0.495358\n",
      "epoch 38, run 20, loss 0.189963\n",
      "epoch 38, run 30, loss 0.430071\n",
      "epoch 38, Validation acc: 42.4%, hit rate: 71.6%, class 0 accuracy: 81.6%\n",
      "epoch 39, run 0, loss 0.61104\n",
      "epoch 39, run 10, loss 0.567432\n",
      "epoch 39, run 20, loss 0.27286\n",
      "epoch 39, run 30, loss 0.383454\n",
      "epoch 39, Validation acc: 42.8%, hit rate: 72%, class 0 accuracy: 81.6%\n",
      "epoch 40, run 0, loss 0.478953\n",
      "epoch 40, run 10, loss 0.576832\n",
      "epoch 40, run 20, loss 0.492543\n",
      "epoch 40, run 30, loss 0.345614\n",
      "epoch 40, Validation acc: 43.6%, hit rate: 73.2%, class 0 accuracy: 80.8%\n",
      "epoch 41, run 0, loss 0.406244\n",
      "epoch 41, run 10, loss 0.75035\n",
      "epoch 41, run 20, loss 0.419945\n",
      "epoch 41, run 30, loss 0.53828\n",
      "epoch 41, Validation acc: 40%, hit rate: 71.2%, class 0 accuracy: 86%\n",
      "epoch 42, run 0, loss 0.318504\n",
      "epoch 42, run 10, loss 0.801728\n",
      "epoch 42, run 20, loss 0.379737\n",
      "epoch 42, run 30, loss 0.370579\n",
      "epoch 42, Validation acc: 41.2%, hit rate: 70%, class 0 accuracy: 86.4%\n",
      "epoch 43, run 0, loss 0.446857\n",
      "epoch 43, run 10, loss 0.830518\n",
      "epoch 43, run 20, loss 0.512097\n",
      "epoch 43, run 30, loss 0.32045\n",
      "epoch 43, Validation acc: 38.8%, hit rate: 68.4%, class 0 accuracy: 89.6%\n",
      "epoch 44, run 0, loss 0.630688\n",
      "epoch 44, run 10, loss 0.828092\n",
      "epoch 44, run 20, loss 0.804717\n",
      "epoch 44, run 30, loss 0.261402\n",
      "epoch 44, Validation acc: 38.4%, hit rate: 69.2%, class 0 accuracy: 86.4%\n",
      "epoch 45, run 0, loss 0.317212\n",
      "epoch 45, run 10, loss 0.426756\n",
      "epoch 45, run 20, loss 0.850173\n",
      "epoch 45, run 30, loss 0.317479\n",
      "epoch 45, Validation acc: 41.6%, hit rate: 68.4%, class 0 accuracy: 84%\n",
      "epoch 46, run 0, loss 0.195764\n",
      "epoch 46, run 10, loss 0.425339\n",
      "epoch 46, run 20, loss 0.61445\n",
      "epoch 46, run 30, loss 0.354951\n",
      "epoch 46, Validation acc: 37.6%, hit rate: 66%, class 0 accuracy: 89.2%\n",
      "epoch 47, run 0, loss 0.300643\n",
      "epoch 47, run 10, loss 0.738298\n",
      "epoch 47, run 20, loss 1.28816\n",
      "epoch 47, run 30, loss 0.554729\n",
      "epoch 47, Validation acc: 43.6%, hit rate: 76%, class 0 accuracy: 82.4%\n",
      "epoch 48, run 0, loss 0.368957\n",
      "epoch 48, run 10, loss 0.786629\n",
      "epoch 48, run 20, loss 0.736146\n",
      "epoch 48, run 30, loss 0.310668\n",
      "epoch 48, Validation acc: 41.6%, hit rate: 75.2%, class 0 accuracy: 83.6%\n",
      "epoch 49, run 0, loss 0.415213\n",
      "epoch 49, run 10, loss 0.712145\n",
      "epoch 49, run 20, loss 0.641491\n",
      "epoch 49, run 30, loss 0.294277\n",
      "epoch 49, Validation acc: 43.2%, hit rate: 76%, class 0 accuracy: 79.2%\n",
      "epoch 50, run 0, loss 0.457602\n",
      "epoch 50, run 10, loss 0.867952\n",
      "epoch 50, run 20, loss 0.905049\n",
      "epoch 50, run 30, loss 0.200085\n",
      "epoch 50, Validation acc: 37.6%, hit rate: 73.6%, class 0 accuracy: 90.4%\n",
      "epoch 51, run 0, loss 0.412302\n",
      "epoch 51, run 10, loss 0.757505\n",
      "epoch 51, run 20, loss 0.81755\n",
      "epoch 51, run 30, loss 0.317137\n",
      "epoch 51, Validation acc: 45.2%, hit rate: 78%, class 0 accuracy: 70.8%\n",
      "epoch 52, run 0, loss 0.2992\n",
      "epoch 52, run 10, loss 0.69076\n",
      "epoch 52, run 20, loss 0.75205\n",
      "epoch 52, run 30, loss 0.606358\n",
      "epoch 52, Validation acc: 42.8%, hit rate: 76.8%, class 0 accuracy: 64.4%\n",
      "epoch 53, run 0, loss 0.35691\n",
      "epoch 53, run 10, loss 0.611038\n",
      "epoch 53, run 20, loss 0.686356\n",
      "epoch 53, run 30, loss 0.763729\n",
      "epoch 53, Validation acc: 43.2%, hit rate: 76.4%, class 0 accuracy: 56.4%\n",
      "epoch 54, run 0, loss 0.60747\n",
      "epoch 54, run 10, loss 0.618231\n",
      "epoch 54, run 20, loss 0.780232\n",
      "epoch 54, run 30, loss 0.534177\n",
      "epoch 54, Validation acc: 44.4%, hit rate: 77.2%, class 0 accuracy: 60.4%\n",
      "epoch 55, run 0, loss 0.63373\n",
      "epoch 55, run 10, loss 0.80182\n",
      "epoch 55, run 20, loss 0.879834\n",
      "epoch 55, run 30, loss 0.483276\n",
      "epoch 55, Validation acc: 42%, hit rate: 75.2%, class 0 accuracy: 77.2%\n",
      "epoch 56, run 0, loss 0.62993\n",
      "epoch 56, run 10, loss 0.80593\n",
      "epoch 56, run 20, loss 0.795572\n",
      "epoch 56, run 30, loss 0.418704\n",
      "epoch 56, Validation acc: 44%, hit rate: 77.6%, class 0 accuracy: 64.8%\n",
      "epoch 57, run 0, loss 0.427557\n",
      "epoch 57, run 10, loss 0.630743\n",
      "epoch 57, run 20, loss 0.931279\n",
      "epoch 57, run 30, loss 0.549841\n",
      "epoch 57, Validation acc: 44.4%, hit rate: 77.6%, class 0 accuracy: 59.2%\n",
      "epoch 58, run 0, loss 0.291424\n",
      "epoch 58, run 10, loss 0.614261\n",
      "epoch 58, run 20, loss 0.966348\n",
      "epoch 58, run 30, loss 0.488658\n",
      "epoch 58, Validation acc: 44%, hit rate: 77.2%, class 0 accuracy: 60.4%\n",
      "epoch 59, run 0, loss 0.474517\n",
      "epoch 59, run 10, loss 0.495874\n",
      "epoch 59, run 20, loss 0.826699\n",
      "epoch 59, run 30, loss 0.588148\n",
      "epoch 59, Validation acc: 42.8%, hit rate: 77.2%, class 0 accuracy: 72.8%\n",
      "epoch 60, run 0, loss 0.551438\n",
      "epoch 60, run 10, loss 0.539785\n",
      "epoch 60, run 20, loss 0.787178\n",
      "epoch 60, run 30, loss 0.491521\n",
      "epoch 60, Validation acc: 43.6%, hit rate: 77.6%, class 0 accuracy: 71.6%\n",
      "epoch 61, run 0, loss 0.458314\n",
      "epoch 61, run 10, loss 0.472835\n",
      "epoch 61, run 20, loss 0.629093\n",
      "epoch 61, run 30, loss 0.698198\n",
      "epoch 61, Validation acc: 44%, hit rate: 76.8%, class 0 accuracy: 68.4%\n",
      "epoch 62, run 0, loss 0.423906\n",
      "epoch 62, run 10, loss 0.464385\n",
      "epoch 62, run 20, loss 0.389386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-7be38ad25fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# need to incoporate y in the batches and expand to 8 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonghaor/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonghaor/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonghaor/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/gonghaor/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gonghaor/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "for i in range(epochs+1):\n",
    "    xloss = 0\n",
    "    acc = 0.0\n",
    "    \n",
    "    for j in range(total_batch):\n",
    "        # need to incoporate y in the batches and expand to 8 classes \n",
    "        index, x_, y_ = next_batch((train_corpus_embed, train_label), index, batch_size)\n",
    "        _, xloss = sess.run([optimizer, loss], feed_dict={x: x_, y: y_, dropout_rate: 0.5})\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(\"epoch %d, run %d, loss %g\" % (i, j, xloss))\n",
    "            \n",
    "    if i % 1 == 0:\n",
    "        acc, hr, acc_most = sess.run([accuracy, hit_rate, accuracy_most_likely_class], feed_dict={x:valid_corpus_embed, y:valid_label, dropout_rate: 1.0, comp: most_class})\n",
    "        print(\"epoch %d, Validation acc: %g\" % (i, acc * 100), end=\"\")\n",
    "        print(\"%\", end=\"\")\n",
    "        print(\", hit rate: %g\" % (hr *100), end=\"\")\n",
    "        print(\"%\", end=\"\")\n",
    "        print(\", class 0 accuracy: %g\" % (acc_most *100), end=\"\")\n",
    "        print(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the benefits and downsides of the RNN-based representation over the bag of words representation used last week? \n",
    "- How would availability of data affect your answer?\n",
    "- One possible architectural variant is to use only the final hidden state of the RNN as the document representation (i.e., x) rather than the average of the hidden states over time. How does this work? What are the potential benefits and downsides to this representation?\n",
    "- Try different RNN architectures, e.g., simple Elman RNNs or GRUs or LSTMs. Which ones work best?\n",
    "- What happens if you use a bidirectional LSTM (i.e., the dashed arrows in the figure)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
