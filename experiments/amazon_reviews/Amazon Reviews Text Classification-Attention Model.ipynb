{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, TimeDistributed\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Activation, Reshape, merge, Merge\n",
    "from keras.layers import SimpleRNN, GRU, LSTM, Bidirectional\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = np.load('data_and_embedding.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = int(f['num_words'])\n",
    "embedding_dim = int(f['embedding_dim'])\n",
    "max_sequence_length = int(f['max_sequence_length'])\n",
    "max_sentence_length = 107 #int(f['max_sentence_length'])\n",
    "\n",
    "data = f['data']\n",
    "labels = f['labels']\n",
    "\n",
    "embedding_matrix = f['embedding_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_split = 0.2 \n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(validation_split * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Reference codes from Dusans \n",
    "\n",
    "# def get_birnn_attention(max_sequence_length, outputs=1):\n",
    "#     input_layer = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "#     embedded_sequences = embedding_layer(input_layer)\n",
    "\n",
    "#     bidir_layer = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
    "#     print(bidir_layer.shape)\n",
    "\n",
    "#     attlayer = TimeDistributed(Dense(100), input_shape=(max_sequence_length,))(bidir_layer)  # 2nd dimension does not matter here\n",
    "#     attlayer = TimeDistributed(Activation('tanh'))(attlayer)\n",
    "#     attlayer = TimeDistributed(Dense(1))(attlayer)\n",
    "#     attlayer = Flatten()(attlayer)\n",
    "#     attlayer = Activation('softmax')(attlayer)\n",
    "#     attlayer = Reshape((-1,1), name='attention_weights')(attlayer)\n",
    "#     print(attlayer.shape)\n",
    "#     context_layer = merge([bidir_layer, attlayer], mode = lambda x: K.batch_dot(x[0], x[1], axes=[1,1]), output_shape= lambda x: (x[0][0], x[0][2]))\n",
    "# #     final_model = merge([ll, attlayer], mode = lambda x: K.dot(K.transpose(x[0]), x[1]), output_shape= lambda x: (x[0][0],x[0][2]))\n",
    "#     print(context_layer.shape)\n",
    "#     return context_layer\n",
    "\n",
    "# get_birnn_attention(101)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sequence_input = Input(shape=(max_sentence_length, max_sequence_length, ), dtype='int32')\n",
    "# # print(sequence_input.shape)\n",
    "\n",
    "# x = TimeDistributed(embedding_layer)(sequence_input)\n",
    "\n",
    "# x = TimeDistributed(Bidirectional(GRU(50, return_sequences=True)))(x)\n",
    "# print(x)\n",
    "\n",
    "# word_attn = TimeDistributed(TimeDistributed(Dense(100), input_shape=(max_sequence_length,)))(x)\n",
    "\n",
    "# word_attn = TimeDistributed(TimeDistributed(Activation('tanh')))(word_attn)\n",
    "\n",
    "# word_attn = TimeDistributed(TimeDistributed(Dense(1)))(word_attn)\n",
    "\n",
    "# word_attn = TimeDistributed(Flatten())(word_attn)\n",
    "\n",
    "# word_attn = TimeDistributed(Activation('softmax'))(word_attn)\n",
    "\n",
    "# word_attn = TimeDistributed(Reshape((-1,1)))(word_attn)\n",
    "# print(word_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def atten_dot(x):\n",
    "#     # x[0] has shape (?, 107, 1000, 100), x[1] has shape (?, 107, 1000, 1)\n",
    "#     xshape = x[0].shape\n",
    "#     print('kk')\n",
    "#     vec = np.reshape(x[0], (x[0].shape[0]*x[0].shape[1], x[0].shape[2], x[0].shape[3]))\n",
    "#     wei = np.reshape(x[1], (x[1].shape[0]*x[1].shape[1], x[1].shape[2], x[1].shape[3]))\n",
    "#     comp = np.multiply(vec, wei)\n",
    "#     compshape = comp.shape\n",
    "# #     print(compshape)\n",
    "#     comp_reshaped = tf.reshape(comp, (xshape[0], xshape[1], compshape[1], compshape[2]))\n",
    "# #     print(comp_reshaped.shape)\n",
    "#     re = tf.reduce_mean(comp_reshaped, axis=2)\n",
    "# #     print(re.shape)\n",
    "#     return re\n",
    "\n",
    "# def outputshape(input_shape):\n",
    "#     print(type(input_shape))\n",
    "#     shape = list(input_shape)\n",
    "# #     assert len(shape)==2\n",
    "#     outshape = (shape[0][0], shape[0][1], shape[0][3])\n",
    "#     return tuple(outshape) \n",
    "\n",
    "# # print(atten_dot([np.ones((2,107,1000,100)), np.ones((2,107,1000,1))]))\n",
    "\n",
    "# sentence_attn = TimeDistributed(Merge([x, word_attn], mode = atten_dot, output_shape = outputshape ))\n",
    "\n",
    "# # Lambda(embedding_mean)\n",
    "\n",
    "\n",
    "# # sentence_attn = TimeDistributed()()\n",
    "\n",
    "\n",
    "\n",
    "# # word_level_att = TimeDistributed(get_birnn_attention(max_sentence_length))(sequence_input)\n",
    "\n",
    "# # sentence_level_att = get_birnn_attention(max_sequence_length)(word_level_att)\n",
    "\n",
    "# # preds = Dense(6, activation='softmax')(sentence_level_att)\n",
    "\n",
    "# # model_attention = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, output_dim=None, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "#         self.init = initializations.get('normal')\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "#         assert len(input_shape) == 3\n",
    "#         self.W = self.init((input_shape[-1], 1))\n",
    "#         self.trainable_weights = [self.W]\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[-1], 1),\n",
    "                                     initializer='normal',\n",
    "                                     trainable=True)\n",
    "        \n",
    "        super(AttLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.kernel))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = tf.expand_dims(ai/K.sum(ai, axis=1), -1) #ai/K.sum(ai, axis=1).dimshuffle(0, 'x')\n",
    "        \n",
    "        weighted_input = tf.expand_dims(x*weights, -1) #x*weights.dimshuffle(0, 1, 'x')\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "    \n",
    "#     def get_output_shape_for(self, input_shape):\n",
    "#         return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_input = Input(shape=(max_sequence_length, ), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "gru_word = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
    "dense_word = TimeDistributed(Dense(100))(gru_word)\n",
    "att_word = AttLayer()(dense_word)\n",
    "sentEncoder = Model(sentence_input, att_word)\n",
    "\n",
    "review_input = Input(shape=(max_sentence_length, max_sequence_length), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "gru_sent = Bidirectional(GRU(50, return_sequences=True))(review_encoder)\n",
    "dense_sent = TimeDistributed(Dense(100))(gru_sent)\n",
    "att_sent = AttLayer()(dense_sent)\n",
    "preds = Dense(6, activation='softmax')(att_sent)\n",
    "model_attention = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 107, 1000)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_87 (TimeDis (None, 107, 100)          2055500   \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 107, 100)          45300     \n",
      "_________________________________________________________________\n",
      "time_distributed_88 (TimeDis (None, 107, 100)          10100     \n",
      "_________________________________________________________________\n",
      "att_layer_7 (AttLayer)       (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 2,111,606.0\n",
      "Trainable params: 111,606.0\n",
      "Non-trainable params: 2,000,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_attention.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_attention.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "model_attention.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "print(\"Training time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv_rnn.save('models/Attention.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
