{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import data.load\n",
    "from metrics.accuracy import conlleval\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load Data\n",
    "train_set, valid_set, dicts = data.load.atisfull()\n",
    "w2idx, ne2idx, labels2idx = dicts['words2idx'], dicts['tables2idx'], dicts['labels2idx']\n",
    "\n",
    "# Create index to word/label dicts\n",
    "idx2w  = {w2idx[k]:k for k in w2idx}\n",
    "idx2ne = {ne2idx[k]:k for k in ne2idx}\n",
    "idx2la = {labels2idx[k]:k for k in labels2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Model\n",
    "n_classes = len(idx2la)\n",
    "n_vocab = len(idx2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Convolution1D(64,5,border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Ground truths etc for conlleval\n",
    "train_x, train_ne, train_label = train_set\n",
    "val_x, val_ne, val_label = valid_set\n",
    "\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in val_x]\n",
    "groundtruth_val = [ list(map(lambda x: idx2la[x], y)) for y in val_label]\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in train_x]\n",
    "groundtruth_train = [ list(map(lambda x: idx2la[x], y)) for y in train_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training\n",
    "n_epochs = 100\n",
    "\n",
    "train_f_scores = []\n",
    "val_f_scores = []\n",
    "best_val_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    \n",
    "    print(\"Training =>\")\n",
    "    train_pred_label = []\n",
    "    avgLoss = 0\n",
    "    \n",
    "    bar = progressbar.ProgressBar(max_value=len(train_x))\n",
    "    for n_batch, sent in bar(enumerate(train_x)):\n",
    "        label = train_label[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.train_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        train_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_train = [ list(map(lambda x: idx2la[x], y)) for y in train_pred_label]\n",
    "    con_dict = conlleval(predword_train, groundtruth_train, words_train, 'r.txt')\n",
    "    train_f_scores.append(con_dict['f1'])\n",
    "    print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "    \n",
    "    \n",
    "    print(\"Validating =>\")\n",
    "    \n",
    "    val_pred_label = []\n",
    "    avgLoss = 0\n",
    "    \n",
    "    bar = progressbar.ProgressBar(max_value=len(val_x))\n",
    "    for n_batch, sent in bar(enumerate(val_x)):\n",
    "        label = val_label[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.test_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        val_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_val = [ list(map(lambda x: idx2la[x], y)) for y in val_pred_label]\n",
    "    con_dict = conlleval(predword_val, groundtruth_val, words_val, 'r.txt')\n",
    "    val_f_scores.append(con_dict['f1'])\n",
    "    \n",
    "    print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "\n",
    "    if con_dict['f1'] > best_val_f1:\n",
    "        best_val_f1 = con_dict['f1']\n",
    "        open('model_architecture.json','w').write(model.to_json())\n",
    "        model.save_weights('best_model_weights.h5',overwrite=True)\n",
    "        print(\"Best validation F1 score = {}\".format(best_val_f1))\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
